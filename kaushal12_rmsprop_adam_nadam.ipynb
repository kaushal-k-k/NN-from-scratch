{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaushal12 rmsprop_adam_nadam",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "17TK1Joi27LD"
      },
      "source": [
        "#Q1\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "dummy = []\r\n",
        "for i in range (10):\r\n",
        "  for j in range(len(Y_train)):\r\n",
        "    if int(Y_train[j]) is i:\r\n",
        "      plt.imshow(X_train[j], cmap=\"gray\")\r\n",
        "      dummy.append(X_train[j])\r\n",
        "      plt.show()\r\n",
        "      break \r\n",
        "wandb.log(({\"Q1\":[wandb.Image(i,caption=j) for i,j in zip(dummy,names)]}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tObnr5Vw9pL5"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnZlpnCx5GyU"
      },
      "source": [
        "def sgd(run,n_epoch,batch_size=100,eta = 0.01):\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "\r\n",
        "      if(j%batch_size == 0):\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        W = add(W , d_w , 1, -1*eta)\r\n",
        "        b = add(b , d_b , 1, -1*eta)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    # print(W,b)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i) \r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def momentum_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.5):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  p_w , p_b =mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b =mul(W , 0) , mul(b,0)\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)   \r\n",
        "    \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_w , d_w , gamma , eta) , add(p_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        p_w , p_b = v_w , v_b\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcael1Q351mq"
      },
      "source": [
        "def onehot_encoding(a,n_class):   \r\n",
        "  temp = []\r\n",
        "  for i in a:\r\n",
        "    t1 = np.zeros(n_class)\r\n",
        "    t1[i] = 1\r\n",
        "    temp.append(t1)\r\n",
        "  return temp\r\n",
        "\r\n",
        "def squr(d):    \r\n",
        "  temp = {}\r\n",
        "  for i in d.keys():\r\n",
        "    temp[i] = d[i]**2\r\n",
        "  return temp\r\n",
        "\r\n",
        "def mul(d1 , m1 = 1):   \r\n",
        "  temp ={}\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i]\r\n",
        "  return temp\r\n",
        "\r\n",
        "# dictionary add key wise\r\n",
        "def add(d1,d2,m1=1,m2=1):\r\n",
        "  temp ={}\r\n",
        "  # print(d1,d2,\"dd\")\r\n",
        "  if (m2==0):\r\n",
        "    return d1\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i] + m2 * d2[i]\r\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSgluWF6gdi"
      },
      "source": [
        "def nesterov_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.9):\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  p_v_w , p_v_b = mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],add(W,v_w,1,-gamma),add(b,v_b,1,-gamma))\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_v_w , d_w , gamma , eta) , add(p_v_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        p_v_w , p_v_b = v_w , v_b\r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def adagrad(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(v_w, squr(d_w)) , add(v_b , squr(d_b))\r\n",
        "        W = add(W , adarate(d_w,v_w,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(d_b,v_b,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)    \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQyFs4w17Sj6"
      },
      "source": [
        "# y_o original output y_p predicted output\r\n",
        "def cross_entropy(y_o,y_p):\r\n",
        "  for i,j in zip(y_o,y_p):\r\n",
        "    if (i==1 and j >= 0):\r\n",
        "      return -1*np.log(1e-15+j,dtype=np.float128) + regularize_loss()\r\n",
        "  return 1e+3 \r\n",
        "\r\n",
        "#  gradient for the oputput layer when cross entropy is used\r\n",
        "def grad_cross_entropy(y_o,y_p):#PMS\r\n",
        "  return -(y_o - y_p) \r\n",
        "\r\n",
        "def regularize_loss():\r\n",
        "  global W,lambda1\r\n",
        "  temp = squr(W)\r\n",
        "  # print(temp)\r\n",
        "  total = 0\r\n",
        "  for i in temp.keys():\r\n",
        "    total = total + np.sum(temp[i])\r\n",
        "\r\n",
        "  return lambda1 * total\r\n",
        "# regularize_loss()\r\n",
        "\r\n",
        "def squared_error(y_o,y_p):#PMS\r\n",
        "  return  np.sum((np.array(y_o)-np.array(y_p))**2) + regularize_loss()\r\n",
        "\r\n",
        "def grad_squared_error(y_o,y_p):#PMS\r\n",
        "  # print(y_o,y_p)\r\n",
        "  y_o = list(y_o)\r\n",
        "  y_p = list(y_p)\r\n",
        "  temp =[]\r\n",
        "  ind = y_o.index(max(y_o))\r\n",
        "  for i in range(len(y_o)):\r\n",
        "    temp.append(-1* 2 * y_p[ind] * (y_o[i]-y_p[ind]) * (y_o[i]-y_p[i]))\r\n",
        "\r\n",
        "  return np.array(temp)\r\n",
        "# grad_squared_error([1,0,0],[0.1,0.2,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQec1oCi8usa"
      },
      "source": [
        "# a is list\r\n",
        "def softmax(a):\r\n",
        "  p = []\r\n",
        "  for i in a:\r\n",
        "    if(i <= 1e+4 ):\r\n",
        "      if(i >= -1e+4):\r\n",
        "        p.append(np.exp(i,dtype=np.float128))\r\n",
        "      else:\r\n",
        "        p.append(0)\r\n",
        "    else:\r\n",
        "      return a/np.sum(a)\r\n",
        "  if(np.sum(p) == 0):\r\n",
        "    return p\r\n",
        "  return p/np.sum(p)\r\n",
        "\r\n",
        "def gradient(x,y,W,b):  \r\n",
        "  global arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "  # forward pass\r\n",
        "  a , h = forward_pass(x,W,b)\r\n",
        "  # print(a,h)\r\n",
        "  # output activation\r\n",
        "  p = globals()[arguments[1]](a[n_hidden_layers])\r\n",
        "\r\n",
        "  # loss\r\n",
        "  l = globals()[arguments[2]](y,p)\r\n",
        "\r\n",
        "  # backward pass\r\n",
        "  d_w ,d_b = backward_pass(x,y,p,a,h,W,b)\r\n",
        "  # print(d_w , d_b)\r\n",
        "\r\n",
        "  return d_w , d_b , l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlQ7nJ2d85Vi"
      },
      "source": [
        "def forward_pass(X,W,b):  \r\n",
        "  global arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "  # h does not contain X\r\n",
        "  h={}\r\n",
        "  a={}\r\n",
        "  # input \r\n",
        "  a[0] = np.array(X @ W[0] + b[0],dtype=np.float32)\r\n",
        "  # print(a[0])\r\n",
        "  \r\n",
        "  \r\n",
        "  h[0] = list(map(lambda x : globals()[arguments[0]](x),a[0]))\r\n",
        "  # print(h[0])\r\n",
        "\r\n",
        "  # hidden\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    a[i] = np.array(h[i-1] @ W[i] + b[i])\r\n",
        "    h[i] = list(map(lambda x : globals()[arguments[0]](x),a[i]))\r\n",
        "\r\n",
        "  # output\r\n",
        "  a[n_hidden_layers] = np.array(h[n_hidden_layers-1] @ W[n_hidden_layers] + b[n_hidden_layers])\r\n",
        "\r\n",
        "  # print(\"a\",a,\"h\")\r\n",
        "\r\n",
        "  return a , h\r\n",
        "\r\n",
        "\r\n",
        "# forward_pass(np.random.rand(n_input),n_input,n_output=4,n_hidden_layers=2,n_hidden_neurons=[2,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAI2YIx_9QHS"
      },
      "source": [
        "def backward_pass(x,y,p,a,h,W,b):\r\n",
        "  global arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons,lambda1\r\n",
        "  d_al = None\r\n",
        "  d_h = None\r\n",
        "  total_layers = n_hidden_layers + 2\r\n",
        "  inner_activation = arguments[0]\r\n",
        "  output_activation = arguments[2]\r\n",
        "\r\n",
        "  # gradient for output w.r.t a_l\r\n",
        "  # print(\"o\",y)\r\n",
        "  d_al = globals()[\"grad_\"+output_activation](y,p)\r\n",
        "\r\n",
        "  d_w ={}\r\n",
        "  d_b ={}\r\n",
        "  #  for all hidden layers\r\n",
        "  for i in range(n_hidden_layers,0,-1):\r\n",
        "    d_w[i] = np.array([np.dot(d_al,h[i-1][j]) for j in range(n_hidden_neurons[i-1])])\r\n",
        "    \r\n",
        "    d_b[i] = np.array(d_al)\r\n",
        "\r\n",
        "    d_h = np.array((np.matrix(W[i]) @ np.matrix(d_al).T).T)[0]\r\n",
        "    # print(\"d_h\",d_h)\r\n",
        "\r\n",
        "    d_al = [d_h[j] * globals()[\"grad_\"+inner_activation](a[i-1][j]) for j in range(n_hidden_neurons[i-1])]\r\n",
        "\r\n",
        "  # for input layer\r\n",
        "  d_w[0] = np.array([np.dot(d_al,x[j]) for j in range(n_input)])\r\n",
        "  d_b[0] =  np.array(d_al)\r\n",
        "\r\n",
        "  return add(d_w,W,1,lambda1) , d_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g69gPkhT9WzZ"
      },
      "source": [
        "def create_network_random():  #PMS\r\n",
        "  # input + output = 2\r\n",
        "  total_layers = 2 + n_hidden_layers \r\n",
        "  W = {}\r\n",
        "  b = {}\r\n",
        "  #  for xavier t1 = 1/np.squrt((ni no of input neurons +no of output neurons))\r\n",
        "  t1 = 1\r\n",
        "  # initialization for W0 and b0 i.e. input layer\r\n",
        "  W[0] = np.random.randn(n_input,n_hidden_neurons[0])/t1\r\n",
        "  b[0] = np.random.randn(n_hidden_neurons[0])/t1\r\n",
        "  # print(type(W[0]))\r\n",
        "\r\n",
        "  # hidden layer\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    W[i] = np.random.randn(n_hidden_neurons[i-1],n_hidden_neurons[i])/t1\r\n",
        "    b[i] = np.random.randn(n_hidden_neurons[i])/t1\r\n",
        "    \r\n",
        "\r\n",
        "  # output layer\r\n",
        "  W[total_layers-2] = np.random.randn(n_hidden_neurons[-1],n_output)/t1\r\n",
        "  b[total_layers-2] = np.random.randn(n_output)/t1\r\n",
        "\r\n",
        "  # print(\"W\",len(W))\r\n",
        "  # for i in W.values():\r\n",
        "  #   print(np.shape(i)) \r\n",
        "  # print(\"b\",len(b))\r\n",
        "  # for i in b.values():\r\n",
        "  #   print(len(i))\r\n",
        "\r\n",
        "  return W,b\r\n",
        "# create_network()\r\n",
        "def create_network_xavier():  #PMS\r\n",
        "  # input + output = 2\r\n",
        "  total_layers = 2 + n_hidden_layers \r\n",
        "  W = {}\r\n",
        "  b = {}\r\n",
        "  #  for xavier t1 = 1/np.squrt((ni no of input neurons +no of output neurons))\r\n",
        "  t1 = 1\r\n",
        "  # initialization for W0 and b0 i.e. input layer\r\n",
        "  W[0] = np.random.rand(n_input,n_hidden_neurons[0]) * np.sqrt((n_input+n_hidden_neurons[0]))\r\n",
        "  b[0] = np.random.rand(n_hidden_neurons[0]) * 0\r\n",
        "  # print(type(W[0])\r\n",
        "\r\n",
        "  # hidden layer\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    W[i] = np.random.rand(n_hidden_neurons[i-1],n_hidden_neurons[i]) * np.sqrt((n_hidden_neurons[i-1] + n_hidden_neurons[i]))\r\n",
        "    b[i] = np.random.rand(n_hidden_neurons[i]) * 0\r\n",
        "    \r\n",
        "\r\n",
        "  # output layer\r\n",
        "  W[total_layers-2] = np.random.rand(n_hidden_neurons[-1],n_output) * np.sqrt((n_hidden_neurons[-1] + n_output))\r\n",
        "  b[total_layers-2] = np.random.rand(n_output) * 0\r\n",
        "\r\n",
        "  # print(\"W\",len(W))\r\n",
        "  # for i in W.values():\r\n",
        "  #   print(np.shape(i)) \r\n",
        "  # print(\"b\",len(b))\r\n",
        "  # for i in b.values():\r\n",
        "  #   print(len(i))\r\n",
        "\r\n",
        "  return W,b\r\n",
        "# create_network_xavier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4oOZTcP94lJ"
      },
      "source": [
        "def accuracy(y_o, y_p):\r\n",
        "  sum = 0\r\n",
        "  for i,j in zip(y_o , y_p):\r\n",
        "    if(i == j):\r\n",
        "      sum = sum + 1\r\n",
        "  \r\n",
        "  return sum/len(y_o)\r\n",
        "def predict_and_loss(X,Y,n):\r\n",
        "  global W,b,arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "\r\n",
        "  loss = 0\r\n",
        "  predicted_class = []\r\n",
        "  for i in range(n):\r\n",
        "    # forward pass\r\n",
        "    a , h = forward_pass(X[i],W,b)\r\n",
        "    # if(i==0):\r\n",
        "    #   print(\"a\",a,\"h\",h)\r\n",
        "    # output activation\r\n",
        "    p = globals()[arguments[1]](a[n_hidden_layers])\r\n",
        "    p = list(p)\r\n",
        "    # print(p)\r\n",
        "    predicted_class.append(p.index(max(p)))\r\n",
        "\r\n",
        "    #  loss\r\n",
        "    ll = globals()[arguments[2]](Y[i],p)\r\n",
        "    if(np.isnan(ll) or np.isinf(ll)):\r\n",
        "      loss = loss + 1e+100\r\n",
        "    else:\r\n",
        "      loss = loss + ll\r\n",
        "\r\n",
        "  return predicted_class , loss/n\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_and_loss(X,Y,y):\r\n",
        "  n_points , _ = np.shape(X)\r\n",
        "  # print(len(Y))\r\n",
        "  p , l = predict_and_loss(X,Y,n_points)\r\n",
        "  # print(p)\r\n",
        "  acc  = accuracy(y, p)\r\n",
        "\r\n",
        "  return acc , l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoaiVom-dFR"
      },
      "source": [
        "\r\n",
        "# n single vale\r\n",
        "def sigmoid(n):   \r\n",
        "  if(n >= 1e+2 ):\r\n",
        "    return 1\r\n",
        "  elif (n <= 1e-2):\r\n",
        "    return 1e-3\r\n",
        "  return 1/(1 + np.exp(-n,dtype= np.float128))\r\n",
        "# print(sigmoid(3))\r\n",
        "# s = [1,2,3]\r\n",
        "# t={}\r\n",
        "# t[0]=list(map(lambda x : sigmoid(x),s))\r\n",
        "# print(t)\r\n",
        "# single value n \r\n",
        "def grad_sigmoid(n):\r\n",
        "  temp = sigmoid(n)\r\n",
        "  return temp * (1 - temp)\r\n",
        "def Relu(n):  \r\n",
        "  if n <= 0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return n\r\n",
        "def grad_Relu(n):\r\n",
        "  if n <= 0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return 1\r\n",
        "def tanh(n):\r\n",
        "  if ( n >= 1e+2):\r\n",
        "    return 1\r\n",
        "  elif (n <= -1e+2):\r\n",
        "    return -1\r\n",
        "  else:\r\n",
        "    return (np.exp(n,dtype=np.float128) - np.exp(-1*n)) / (np.exp(n) + np.exp(-1*n))\r\n",
        "  return 0\r\n",
        "def grad_tanh(n):\r\n",
        "  return 1-np.power(tanh(n),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfEMWkTZ-yp4"
      },
      "source": [
        "def rmsprop(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1=0.9):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta1,(1-beta1)) , add(v_b , squr(d_b),beta1,(1-beta1))\r\n",
        "        W = add(W , adarate(d_w,v_w,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(d_b,v_b,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return train_loss , train_acc , val_loss, val_acc \r\n",
        "def adam(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  m_w , m_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  time_stamp = 0\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        \r\n",
        "        m_w , m_b = add(m_w,d_w,beta1 ,(1-beta1)) , add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta2,(1-beta2)) , add(v_b , squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "        m_w_hat , m_b_hat = mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "        v_w_hat , v_b_hat = mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "        W = add(W , adarate(m_w_hat,v_w_hat,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(m_b_hat,v_b_hat,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        time_stamp = time_stamp+1\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return train_loss , train_acc , val_loss, val_acc   \r\n",
        "\r\n",
        "        \r\n",
        "# np.seterr(divide='ignore', invalid='ignore')\r\n",
        "def nadam(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  m_w , m_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  time_stamp=0\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],add(W,m_w,1,-beta1),add(b,m_b,1,-beta1))\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        m_w , m_b = add(m_w,d_w,beta1 ,(1-beta1)) , add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta2,(1-beta2)) , add(v_b , squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "        m_w_hat , m_b_hat = mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "        v_w_hat , v_b_hat = mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "        W = add(W , adarate(add(m_w_hat,d_w,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_w_hat,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(add(m_b_hat,d_b,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_b_hat,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        time_stamp = time_stamp +1\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "\r\n",
        "  return train_loss , train_acc , val_loss, val_acc   \r\n",
        "\r\n",
        "        \r\n",
        "# np.seterr(divide='ignore', invalid='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}