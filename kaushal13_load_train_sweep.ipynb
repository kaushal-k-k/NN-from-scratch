{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaushal13 load train sweep",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "17TK1Joi27LD"
      },
      "source": [
        "#Q1\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "dummy = []\r\n",
        "for i in range (10):\r\n",
        "  for j in range(len(Y_train)):\r\n",
        "    if int(Y_train[j]) is i:\r\n",
        "      plt.imshow(X_train[j], cmap=\"gray\")\r\n",
        "      dummy.append(X_train[j])\r\n",
        "      plt.show()\r\n",
        "      break \r\n",
        "wandb.log(({\"Q1\":[wandb.Image(i,caption=j) for i,j in zip(dummy,names)]}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tObnr5Vw9pL5"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnZlpnCx5GyU"
      },
      "source": [
        "def sgd(run,n_epoch,batch_size=100,eta = 0.01):\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "\r\n",
        "      if(j%batch_size == 0):\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        W = add(W , d_w , 1, -1*eta)\r\n",
        "        b = add(b , d_b , 1, -1*eta)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    # print(W,b)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i) \r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def momentum_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.5):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  p_w , p_b =mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b =mul(W , 0) , mul(b,0)\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)   \r\n",
        "    \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_w , d_w , gamma , eta) , add(p_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        p_w , p_b = v_w , v_b\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcael1Q351mq"
      },
      "source": [
        "def onehot_encoding(a,n_class):   \r\n",
        "  temp = []\r\n",
        "  for i in a:\r\n",
        "    t1 = np.zeros(n_class)\r\n",
        "    t1[i] = 1\r\n",
        "    temp.append(t1)\r\n",
        "  return temp\r\n",
        "\r\n",
        "def squr(d):    \r\n",
        "  temp = {}\r\n",
        "  for i in d.keys():\r\n",
        "    temp[i] = d[i]**2\r\n",
        "  return temp\r\n",
        "\r\n",
        "def mul(d1 , m1 = 1):   \r\n",
        "  temp ={}\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i]\r\n",
        "  return temp\r\n",
        "\r\n",
        "# dictionary add key wise\r\n",
        "def add(d1,d2,m1=1,m2=1):\r\n",
        "  temp ={}\r\n",
        "  # print(d1,d2,\"dd\")\r\n",
        "  if (m2==0):\r\n",
        "    return d1\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i] + m2 * d2[i]\r\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSgluWF6gdi"
      },
      "source": [
        "def nesterov_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.9):\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  p_v_w , p_v_b = mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],add(W,v_w,1,-gamma),add(b,v_b,1,-gamma))\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_v_w , d_w , gamma , eta) , add(p_v_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        p_v_w , p_v_b = v_w , v_b\r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def adagrad(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(v_w, squr(d_w)) , add(v_b , squr(d_b))\r\n",
        "        W = add(W , adarate(d_w,v_w,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(d_b,v_b,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)    \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQyFs4w17Sj6"
      },
      "source": [
        "# y_o original output y_p predicted output\r\n",
        "def cross_entropy(y_o,y_p):\r\n",
        "  for i,j in zip(y_o,y_p):\r\n",
        "    if (i==1 and j >= 0):\r\n",
        "      return -1*np.log(1e-15+j,dtype=np.float128) + regularize_loss()\r\n",
        "  return 1e+3 \r\n",
        "\r\n",
        "#  gradient for the oputput layer when cross entropy is used\r\n",
        "def grad_cross_entropy(y_o,y_p):#PMS\r\n",
        "  return -(y_o - y_p) \r\n",
        "\r\n",
        "def regularize_loss():\r\n",
        "  global W,lambda1\r\n",
        "  temp = squr(W)\r\n",
        "  # print(temp)\r\n",
        "  total = 0\r\n",
        "  for i in temp.keys():\r\n",
        "    total = total + np.sum(temp[i])\r\n",
        "\r\n",
        "  return lambda1 * total\r\n",
        "# regularize_loss()\r\n",
        "\r\n",
        "def squared_error(y_o,y_p):#PMS\r\n",
        "  return  np.sum((np.array(y_o)-np.array(y_p))**2) + regularize_loss()\r\n",
        "\r\n",
        "def grad_squared_error(y_o,y_p):#PMS\r\n",
        "  # print(y_o,y_p)\r\n",
        "  y_o = list(y_o)\r\n",
        "  y_p = list(y_p)\r\n",
        "  temp =[]\r\n",
        "  ind = y_o.index(max(y_o))\r\n",
        "  for i in range(len(y_o)):\r\n",
        "    temp.append(-1* 2 * y_p[ind] * (y_o[i]-y_p[ind]) * (y_o[i]-y_p[i]))\r\n",
        "\r\n",
        "  return np.array(temp)\r\n",
        "# grad_squared_error([1,0,0],[0.1,0.2,0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQec1oCi8usa"
      },
      "source": [
        "# a is list\r\n",
        "def softmax(a):\r\n",
        "  p = []\r\n",
        "  for i in a:\r\n",
        "    if(i <= 1e+4 ):\r\n",
        "      if(i >= -1e+4):\r\n",
        "        p.append(np.exp(i,dtype=np.float128))\r\n",
        "      else:\r\n",
        "        p.append(0)\r\n",
        "    else:\r\n",
        "      return a/np.sum(a)\r\n",
        "  if(np.sum(p) == 0):\r\n",
        "    return p\r\n",
        "  return p/np.sum(p)\r\n",
        "\r\n",
        "def gradient(x,y,W,b):  \r\n",
        "  global arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "  # forward pass\r\n",
        "  a , h = forward_pass(x,W,b)\r\n",
        "  # print(a,h)\r\n",
        "  # output activation\r\n",
        "  p = globals()[arguments[1]](a[n_hidden_layers])\r\n",
        "\r\n",
        "  # loss\r\n",
        "  l = globals()[arguments[2]](y,p)\r\n",
        "\r\n",
        "  # backward pass\r\n",
        "  d_w ,d_b = backward_pass(x,y,p,a,h,W,b)\r\n",
        "  # print(d_w , d_b)\r\n",
        "\r\n",
        "  return d_w , d_b , l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlQ7nJ2d85Vi"
      },
      "source": [
        "def forward_pass(X,W,b):  \r\n",
        "  global arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "  # h does not contain X\r\n",
        "  h={}\r\n",
        "  a={}\r\n",
        "  # input \r\n",
        "  a[0] = np.array(X @ W[0] + b[0],dtype=np.float32)\r\n",
        "  # print(a[0])\r\n",
        "  \r\n",
        "  \r\n",
        "  h[0] = list(map(lambda x : globals()[arguments[0]](x),a[0]))\r\n",
        "  # print(h[0])\r\n",
        "\r\n",
        "  # hidden\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    a[i] = np.array(h[i-1] @ W[i] + b[i])\r\n",
        "    h[i] = list(map(lambda x : globals()[arguments[0]](x),a[i]))\r\n",
        "\r\n",
        "  # output\r\n",
        "  a[n_hidden_layers] = np.array(h[n_hidden_layers-1] @ W[n_hidden_layers] + b[n_hidden_layers])\r\n",
        "\r\n",
        "  # print(\"a\",a,\"h\")\r\n",
        "\r\n",
        "  return a , h\r\n",
        "\r\n",
        "\r\n",
        "# forward_pass(np.random.rand(n_input),n_input,n_output=4,n_hidden_layers=2,n_hidden_neurons=[2,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAI2YIx_9QHS"
      },
      "source": [
        "def backward_pass(x,y,p,a,h,W,b):\r\n",
        "  global arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons,lambda1\r\n",
        "  d_al = None\r\n",
        "  d_h = None\r\n",
        "  total_layers = n_hidden_layers + 2\r\n",
        "  inner_activation = arguments[0]\r\n",
        "  output_activation = arguments[2]\r\n",
        "\r\n",
        "  # gradient for output w.r.t a_l\r\n",
        "  # print(\"o\",y)\r\n",
        "  d_al = globals()[\"grad_\"+output_activation](y,p)\r\n",
        "\r\n",
        "  d_w ={}\r\n",
        "  d_b ={}\r\n",
        "  #  for all hidden layers\r\n",
        "  for i in range(n_hidden_layers,0,-1):\r\n",
        "    d_w[i] = np.array([np.dot(d_al,h[i-1][j]) for j in range(n_hidden_neurons[i-1])])\r\n",
        "    \r\n",
        "    d_b[i] = np.array(d_al)\r\n",
        "\r\n",
        "    d_h = np.array((np.matrix(W[i]) @ np.matrix(d_al).T).T)[0]\r\n",
        "    # print(\"d_h\",d_h)\r\n",
        "\r\n",
        "    d_al = [d_h[j] * globals()[\"grad_\"+inner_activation](a[i-1][j]) for j in range(n_hidden_neurons[i-1])]\r\n",
        "\r\n",
        "  # for input layer\r\n",
        "  d_w[0] = np.array([np.dot(d_al,x[j]) for j in range(n_input)])\r\n",
        "  d_b[0] =  np.array(d_al)\r\n",
        "\r\n",
        "  return add(d_w,W,1,lambda1) , d_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g69gPkhT9WzZ"
      },
      "source": [
        "def create_network_random():  #PMS\r\n",
        "  # input + output = 2\r\n",
        "  total_layers = 2 + n_hidden_layers \r\n",
        "  W = {}\r\n",
        "  b = {}\r\n",
        "  #  for xavier t1 = 1/np.squrt((ni no of input neurons +no of output neurons))\r\n",
        "  t1 = 1\r\n",
        "  # initialization for W0 and b0 i.e. input layer\r\n",
        "  W[0] = np.random.randn(n_input,n_hidden_neurons[0])/t1\r\n",
        "  b[0] = np.random.randn(n_hidden_neurons[0])/t1\r\n",
        "  # print(type(W[0]))\r\n",
        "\r\n",
        "  # hidden layer\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    W[i] = np.random.randn(n_hidden_neurons[i-1],n_hidden_neurons[i])/t1\r\n",
        "    b[i] = np.random.randn(n_hidden_neurons[i])/t1\r\n",
        "    \r\n",
        "\r\n",
        "  # output layer\r\n",
        "  W[total_layers-2] = np.random.randn(n_hidden_neurons[-1],n_output)/t1\r\n",
        "  b[total_layers-2] = np.random.randn(n_output)/t1\r\n",
        "\r\n",
        "  # print(\"W\",len(W))\r\n",
        "  # for i in W.values():\r\n",
        "  #   print(np.shape(i)) \r\n",
        "  # print(\"b\",len(b))\r\n",
        "  # for i in b.values():\r\n",
        "  #   print(len(i))\r\n",
        "\r\n",
        "  return W,b\r\n",
        "# create_network()\r\n",
        "def create_network_xavier():  #PMS\r\n",
        "  # input + output = 2\r\n",
        "  total_layers = 2 + n_hidden_layers \r\n",
        "  W = {}\r\n",
        "  b = {}\r\n",
        "  #  for xavier t1 = 1/np.squrt((ni no of input neurons +no of output neurons))\r\n",
        "  t1 = 1\r\n",
        "  # initialization for W0 and b0 i.e. input layer\r\n",
        "  W[0] = np.random.rand(n_input,n_hidden_neurons[0]) * np.sqrt((n_input+n_hidden_neurons[0]))\r\n",
        "  b[0] = np.random.rand(n_hidden_neurons[0]) * 0\r\n",
        "  # print(type(W[0])\r\n",
        "\r\n",
        "  # hidden layer\r\n",
        "  for i in range(1,n_hidden_layers):\r\n",
        "    W[i] = np.random.rand(n_hidden_neurons[i-1],n_hidden_neurons[i]) * np.sqrt((n_hidden_neurons[i-1] + n_hidden_neurons[i]))\r\n",
        "    b[i] = np.random.rand(n_hidden_neurons[i]) * 0\r\n",
        "    \r\n",
        "\r\n",
        "  # output layer\r\n",
        "  W[total_layers-2] = np.random.rand(n_hidden_neurons[-1],n_output) * np.sqrt((n_hidden_neurons[-1] + n_output))\r\n",
        "  b[total_layers-2] = np.random.rand(n_output) * 0\r\n",
        "\r\n",
        "  # print(\"W\",len(W))\r\n",
        "  # for i in W.values():\r\n",
        "  #   print(np.shape(i)) \r\n",
        "  # print(\"b\",len(b))\r\n",
        "  # for i in b.values():\r\n",
        "  #   print(len(i))\r\n",
        "\r\n",
        "  return W,b\r\n",
        "# create_network_xavier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4oOZTcP94lJ"
      },
      "source": [
        "def accuracy(y_o, y_p):\r\n",
        "  sum = 0\r\n",
        "  for i,j in zip(y_o , y_p):\r\n",
        "    if(i == j):\r\n",
        "      sum = sum + 1\r\n",
        "  \r\n",
        "  return sum/len(y_o)\r\n",
        "def predict_and_loss(X,Y,n):\r\n",
        "  global W,b,arguments,n_input,n_output, n_hidden_layers,n_hidden_neurons\r\n",
        "\r\n",
        "  loss = 0\r\n",
        "  predicted_class = []\r\n",
        "  for i in range(n):\r\n",
        "    # forward pass\r\n",
        "    a , h = forward_pass(X[i],W,b)\r\n",
        "    # if(i==0):\r\n",
        "    #   print(\"a\",a,\"h\",h)\r\n",
        "    # output activation\r\n",
        "    p = globals()[arguments[1]](a[n_hidden_layers])\r\n",
        "    p = list(p)\r\n",
        "    # print(p)\r\n",
        "    predicted_class.append(p.index(max(p)))\r\n",
        "\r\n",
        "    #  loss\r\n",
        "    ll = globals()[arguments[2]](Y[i],p)\r\n",
        "    if(np.isnan(ll) or np.isinf(ll)):\r\n",
        "      loss = loss + 1e+100\r\n",
        "    else:\r\n",
        "      loss = loss + ll\r\n",
        "\r\n",
        "  return predicted_class , loss/n\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_and_loss(X,Y,y):\r\n",
        "  n_points , _ = np.shape(X)\r\n",
        "  # print(len(Y))\r\n",
        "  p , l = predict_and_loss(X,Y,n_points)\r\n",
        "  # print(p)\r\n",
        "  acc  = accuracy(y, p)\r\n",
        "\r\n",
        "  return acc , l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoaiVom-dFR"
      },
      "source": [
        "\r\n",
        "# n single vale\r\n",
        "def sigmoid(n):   \r\n",
        "  if(n >= 1e+2 ):\r\n",
        "    return 1\r\n",
        "  elif (n <= 1e-2):\r\n",
        "    return 1e-3\r\n",
        "  return 1/(1 + np.exp(-n,dtype= np.float128))\r\n",
        "# print(sigmoid(3))\r\n",
        "# s = [1,2,3]\r\n",
        "# t={}\r\n",
        "# t[0]=list(map(lambda x : sigmoid(x),s))\r\n",
        "# print(t)\r\n",
        "# single value n \r\n",
        "def grad_sigmoid(n):\r\n",
        "  temp = sigmoid(n)\r\n",
        "  return temp * (1 - temp)\r\n",
        "def Relu(n):  \r\n",
        "  if n <= 0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return n\r\n",
        "def grad_Relu(n):\r\n",
        "  if n <= 0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return 1\r\n",
        "def tanh(n):\r\n",
        "  if ( n >= 1e+2):\r\n",
        "    return 1\r\n",
        "  elif (n <= -1e+2):\r\n",
        "    return -1\r\n",
        "  else:\r\n",
        "    return (np.exp(n,dtype=np.float128) - np.exp(-1*n)) / (np.exp(n) + np.exp(-1*n))\r\n",
        "  return 0\r\n",
        "def grad_tanh(n):\r\n",
        "  return 1-np.power(tanh(n),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfEMWkTZ-yp4"
      },
      "source": [
        "def rmsprop(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1=0.9):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta1,(1-beta1)) , add(v_b , squr(d_b),beta1,(1-beta1))\r\n",
        "        W = add(W , adarate(d_w,v_w,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(d_b,v_b,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return train_loss , train_acc , val_loss, val_acc \r\n",
        "def adam(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  m_w , m_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  time_stamp = 0\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        \r\n",
        "        m_w , m_b = add(m_w,d_w,beta1 ,(1-beta1)) , add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta2,(1-beta2)) , add(v_b , squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "        m_w_hat , m_b_hat = mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "        v_w_hat , v_b_hat = mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "        W = add(W , adarate(m_w_hat,v_w_hat,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(m_b_hat,v_b_hat,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        time_stamp = time_stamp+1\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return train_loss , train_acc , val_loss, val_acc   \r\n",
        "\r\n",
        "        \r\n",
        "# np.seterr(divide='ignore', invalid='ignore')\r\n",
        "def nadam(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  m_w , m_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  time_stamp=0\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],add(W,m_w,1,-beta1),add(b,m_b,1,-beta1))\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        m_w , m_b = add(m_w,d_w,beta1 ,(1-beta1)) , add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "        v_w , v_b = add(v_w, squr(d_w),beta2,(1-beta2)) , add(v_b , squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "        m_w_hat , m_b_hat = mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "        v_w_hat , v_b_hat = mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "        W = add(W , adarate(add(m_w_hat,d_w,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_w_hat,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(add(m_b_hat,d_b,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_b_hat,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        time_stamp = time_stamp +1\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "\r\n",
        "  return train_loss , train_acc , val_loss, val_acc   \r\n",
        "\r\n",
        "        \r\n",
        "# np.seterr(divide='ignore', invalid='ignore')\r\n",
        "\r\n",
        "def adarate(d1,d2,n,e):\r\n",
        "  temp = {}\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = (n*d1[i]) /(e+d2[i])**(1/2)\r\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jz2vOII_Lrw"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "def load_data():\r\n",
        " \r\n",
        "  (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "  \r\n",
        "  \r\n",
        "\r\n",
        "  # flatten data\r\n",
        "  x_train = []\r\n",
        "  for j in X_train:\r\n",
        "    x_train.append(j.flatten())\r\n",
        "  x_train=np.array(x_train,dtype = np.float32)/255\r\n",
        "\r\n",
        "  # mean centered\r\n",
        "  temp = []\r\n",
        "  m = np.array(np.mean(x_train,0))\r\n",
        "  for j in x_train:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_train = np.array(temp,dtype = np.float32)\r\n",
        "\r\n",
        "  # print(x_train[0] - x_train[1])\r\n",
        "  x_test = []\r\n",
        "  for j in X_test:\r\n",
        "   x_test.append(j.flatten())\r\n",
        "  x_test=np.array(x_test,dtype = np.float32)/255\r\n",
        "\r\n",
        "  temp = []\r\n",
        "  for j in x_test:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_test = np.array(temp,dtype = np.float32)\r\n",
        "  # print(x_train.shape,x_test.shape)\r\n",
        "\r\n",
        "  x_train , vx_test , y_train , vy_test = train_test_split(x_train,Y_train,test_size=0.10,random_state=12)\r\n",
        "  return x_train , onehot_encoding(y_train,10), vx_test , onehot_encoding(vy_test , 10) , y_train , vy_test, x_test,onehot_encoding(Y_test , 10), Y_test\r\n",
        "# sweep\r\n",
        "dcf = [{'activations': 'ReLU', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'adam', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'adam', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'nesterov', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'nesterov', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'rmsprop', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'nesterov', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'nesterov', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'nesterov', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'nesterov', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'adam', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 5, 'optimizer': 'momentum', 'weight_decay': 0}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 5, 'optimizer': 'sgd', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 3, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 3, 'optimizer': 'momentum', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'sgd', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'rmsprop', 'weight_decay': 0.5}, {'activations': 'sigmoid', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 64, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0}, {'activations': 'sigmoid', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'tanh', 'batch_size': 64, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'nesterov', 'weight_decay': 0.0005}, {'activations': 'ReLU', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 5, 'optimizer': 'adam', 'weight_decay': 0.5}, {'activations': 'tanh', 'batch_size': 32, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'squared_error', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'adam', 'weight_decay': 0.0005}, {'activations': 'sigmoid', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.0001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 32, 'n_hidden_layers': 4, 'optimizer': 'momentum', 'weight_decay': 0}, {'activations': 'ReLU', 'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'loss_function': 'cross_entropy', 'n_hidden_layer_size': 128, 'n_hidden_layers': 3, 'optimizer': 'rmsprop', 'weight_decay': 0.5}]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# from sklearn.datasets import make_blobs\r\n",
        "# XX , YY = make_blobs(n_samples=10000,centers=3,n_features = 2, random_state = 0)\r\n",
        "# X , y_t , X_t ,y_o  = XX ,YY,XX,YY\r\n",
        "# Y = onehot_encoding(y_t,3)\r\n",
        "# Y_t = onehot_encoding(y_t,3)\r\n",
        "# print(X)\r\n",
        "# regularize_para = [0,0.005,0.5]\r\n",
        "\r\n",
        "\r\n",
        "# inner activation , ouput activation , loss\r\n",
        "arguments=[\"sigmoid\",\"softmax\",\"squared_error\"]\r\n",
        "n_hidden_neurons=[64,64]\r\n",
        "X,Y,X_t,Y_t,y_o,y_t,x_test,y_test,y_test_t = load_data()\r\n",
        "# print(np.shape(X),np.shape(X_t),np.shape(x_test),Y[1],Y_t[1],y_o[1],y_t[1],len(y_test))\r\n",
        "n_hidden_layers = len(n_hidden_neurons)\r\n",
        "n_output = 10\r\n",
        "n_points , n_input = np.shape(X)\r\n",
        "lambda1 = 0.005\r\n",
        "\r\n",
        "# W,b = None ,None\r\n",
        "# arguments=None\r\n",
        "# n_hidden_neurons=None\r\n",
        "# X,Y,X_t,Y_t,y_o,y_t,x_test,y_test,y_test_t = load_data()\r\n",
        "print(np.shape(X),np.shape(X_t),np.shape(x_test),Y[1],Y_t[1],y_o[1],y_t[1],len(y_test))\r\n",
        "# n_hidden_layers =None\r\n",
        "# n_output =None\r\n",
        "# # n_points , n_input = 0,0\r\n",
        "# lambda1 = None\r\n",
        "dc = []\r\n",
        "\r\n",
        "def train(n_epoch=1):\r\n",
        "  run = wandb.init()\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t,x_test,y_test,y_test_t , lambda1\r\n",
        "  c = run.config\r\n",
        "  # dc.append(c)\r\n",
        "    #  optimizer activation loss hiddenlayers hiddenlayersize epoch learningrate lambda batchsize \r\n",
        "  name = \"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activations)+\"_l_\"+str(c.loss_function)+\"_hl_\"+str(c.n_hidden_layers)+\"_hls_\"+str(c.n_hidden_layer_size)+\"_ep_\"+str(c.epochs)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_wi_\"+str(c.weight_ini)\r\n",
        "  run.name = name\r\n",
        "  print(name)\r\n",
        "  \r\n",
        "  n_hidden_neurons = [c.n_hidden_layer_size]*c.n_hidden_layers  \r\n",
        "  n_hidden_layers = c.n_hidden_layers \r\n",
        "  lambda1 = c.weight_decay\r\n",
        "  arguments = [c.activations,\"softmax\",c.loss_function]\r\n",
        "\r\n",
        "  W,b = globals()[\"create_network_\"+c.weight_ini]()\r\n",
        "  \r\n",
        "  X,Y,X_t,Y_t,y_o,y_t,x_test,y_test,y_test_t = load_data()\r\n",
        "  n_points , n_input = np.shape(X)\r\n",
        "  n_output = 10\r\n",
        "\r\n",
        "  globals()[c.optimizer](run,c.epochs,c.batch_size,c.learning_rate)\r\n",
        "\r\n",
        "  n_p_t , _ = np.shape(x_test) \r\n",
        "  predicted_classes , test_loss = predict_and_loss(x_test,y_test,n_p_t)\r\n",
        "  # create_heatmap(predicted_classes,y_test_t)\r\n",
        "\r\n",
        "  #np.save(\"drive/My Drive/DL_assignments/assignment1/weight/\"+name+str(\"W\"),W)\r\n",
        "  #np.save(\"drive/My Drive/DL_assignments/assignment1/bias/\"+name+str(\"b\"),b)\r\n",
        "\r\n",
        "  label = [\"T-shirt/top\" ,\"Trouser\" ,\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "  # ytrue ypred\r\n",
        "  # wandb.log(step=1)\r\n",
        "  wandb.sklearn.plot_confusion_matrix(y_test_t, predicted_classes, label)\r\n",
        "  run.finish()\r\n",
        "  \r\n",
        "  return\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ETdwSQ_evh"
      },
      "source": [
        "%pip install wandb -q\r\n",
        "import wandb\r\n",
        "sweep_config={\r\n",
        "    'method' : 'random' ,\r\n",
        "    'metric' : { 'name' : 'v_acc' , 'goal' : 'maximize' } ,\r\n",
        "    'parameters' : {\r\n",
        "        'epochs' : { 'values' : [1,2] },\r\n",
        "        'n_hidden_layers' : {'values' : [3,4,5]},\r\n",
        "        'n_hidden_layer_size' : { 'values' : [32,64,128]},\r\n",
        "        'batch_size' : { 'values' : [16,32,64]},\r\n",
        "        'weight_decay' : { 'values' : [0,0.0005] },\r\n",
        "        'learning_rate' : { 'values' : [1e-3 ,5e-3,1e-4] },\r\n",
        "        'optimizer' : { 'values' : ['sgd','momentum_gradient_descent','nesterov_gradient_descent','rmsprop','adam','nadam'] },\r\n",
        "        'activations' : { 'values' : ['sigmoid','tanh','Relu'] },\r\n",
        "        'loss_function' : {'values' : ['cross_entropy' , 'squared_error']},\r\n",
        "        'weight_ini' : {'values' : ['random' , 'xavier']}\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# train()\r\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"dl_assignment1a1a\",entity=\"sonagara\")\r\n",
        "wandb.agent(sweep_id, train)\r\n",
        "\r\n",
        "import seaborn as sn\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def create_heatmap(y,yo):\r\n",
        "  a = np.reshape(np.zeros(10*10),(10,10))\r\n",
        "  label = [\"T-shirt/top\" ,\"Trouser\" ,\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "  # y-xis predicted ,x-axis actual\r\n",
        "  for i,j in zip(y,yo):\r\n",
        "    a[i][j] = a[i][j] + 1\r\n",
        "  # print(a)\r\n",
        "  sn.set(font_scale=1.4)\r\n",
        "  plt.figure(figsize=(13,10))\r\n",
        "  df_cm = pd.DataFrame(a, index=label, columns=label)\r\n",
        "  sn.color_palette('Blues')\r\n",
        "  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\")\r\n",
        "  # plt.show()\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "  return\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}