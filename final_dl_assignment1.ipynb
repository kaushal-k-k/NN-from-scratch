{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_dl_assignment1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXAktufaNqwZ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BddwesMxjG0"
      },
      "source": [
        "%pip install wandb -q\r\n",
        "import wandb\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import pandas as pd\r\n",
        "random.seed(1)\r\n",
        "\r\n",
        "# \r\n",
        "# sweep_config={\r\n",
        "#     'method' : 'random' ,\r\n",
        "#     'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "#     'parameters' : {\r\n",
        "#         'epochs' : { 'values' : [3,5,7] },\r\n",
        "#         'n_hidden_layers' : {'values' : [3,2]},\r\n",
        "#         'n_hidden_layer_size' : { 'values' : [32,64,128]},\r\n",
        "#         'batch_size' : { 'values' : [32,64]},\r\n",
        "#         'weight_decay' : { 'values' : [0.0005]},\r\n",
        "#         'learning_rate' : { 'values' : [2e-3,1e-4]},\r\n",
        "#         'optimizer' : { 'values' : ['sgd','ngd','mgd','rmsprop','adam','nadam','adagrad'] },\r\n",
        "#         'activations' : { 'values' : ['sigmoid','Relu','tanh'] },\r\n",
        "#         'loss_function' : {'values' : ['cross_entropy']},\r\n",
        "#         'weight_ini' : {'values' : ['random' , 'xavier']}\r\n",
        "#     }\r\n",
        "# }\r\n",
        "\r\n",
        "# sweep config for loss\r\n",
        "sweep_config1={\r\n",
        "    'method' : 'random' ,\r\n",
        "    'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "    'parameters' : {\r\n",
        "        'epochs' : { 'values' : [5] },\r\n",
        "        'n_hidden_layers' : {'values' : [3]},\r\n",
        "        'n_hidden_layer_size' : { 'values' : [32]},\r\n",
        "        'batch_size' : { 'values' : [32]},\r\n",
        "        'weight_decay' : { 'values' : [0.0005]},\r\n",
        "        'learning_rate' : { 'values' : [1e-3]},\r\n",
        "        'optimizer' : { 'values' : ['nadam','ngd'] },\r\n",
        "        'activations' : { 'values' : ['sigmoid'] },\r\n",
        "        'loss_function' : {'values' : ['cross_entropy','squared_error']},\r\n",
        "        'weight_ini' : {'values' : ['random']}\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "# sweep_config={\r\n",
        "#     'method' : 'random' ,\r\n",
        "#     'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "#     'parameters' : {\r\n",
        "#         'epochs' : { 'values' : [3,7] },\r\n",
        "#         'n_hidden_layers' : {'values' : [3,2]},\r\n",
        "#         'n_hidden_layer_size' : { 'values' : [16,56]},\r\n",
        "#         'batch_size' : { 'values' : [50,70]},\r\n",
        "#         'weight_decay' : { 'values' : [0.0003] },\r\n",
        "#         'learning_rate' : { 'values' : [2e-3] },\r\n",
        "#         'optimizer' : { 'values' : ['ngd','rmsprop','adam','nadam','adagrad'] },\r\n",
        "#         'activations' : { 'values' : ['sigmoid','tanh'] },\r\n",
        "#         'loss_function' : {'values' : ['cross_entropy' , 'squared_error']},\r\n",
        "#         'weight_ini' : {'values' : ['random' , 'xavier']}\r\n",
        "#     }\r\n",
        "# }\r\n",
        "\r\n",
        "# sweep_config={\r\n",
        "#     'method' : 'random' ,\r\n",
        "#     'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "#     'parameters' : {\r\n",
        "#         'epochs' : { 'values' : [12,7] },\r\n",
        "#         'n_hidden_layers' : {'values' : [3,2]},\r\n",
        "#         'n_hidden_layer_size' : { 'values' : [16,56]},\r\n",
        "#         'batch_size' : { 'values' : [50,100]},\r\n",
        "#         'weight_decay' : { 'values' : [0.0003] },\r\n",
        "#         'learning_rate' : { 'values' : [2e-3] },\r\n",
        "#         'optimizer' : { 'values' : ['ngd','rmsprop','adam','nadam','adagrad'] },\r\n",
        "#         'activations' : { 'values' : ['sigmoid','tanh','Relu'] },\r\n",
        "#         'loss_function' : {'values' : ['cross_entropy' , 'squared_error']},\r\n",
        "#         'weight_ini' : {'values' : ['random' , 'xavier']}\r\n",
        "#     }\r\n",
        "# }\r\n",
        "\r\n",
        "# sweep_config={\r\n",
        "#     'method' : 'bayes' ,\r\n",
        "#     'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "#     'early_terminate' : {\r\n",
        "#         'type' : 'hyperband' ,\r\n",
        "#         'min_iter' : 5\r\n",
        "#     },\r\n",
        "#     'parameters' : {\r\n",
        "#         'epochs' : { 'values' : [10] },\r\n",
        "#         'n_hidden_layers' : {'values' : [3]},\r\n",
        "#         'n_hidden_layer_size' : { 'values' : [32]},\r\n",
        "#         'batch_size' : { 'values' : [16]},\r\n",
        "#         'weight_decay' : { 'values' : [0.0005] },\r\n",
        "#         'learning_rate' : { 'values' : [5e-3] },\r\n",
        "#         'optimizer' : { 'values' : ['rmsprop'] },\r\n",
        "#         'activations' : { 'values' : ['Relu'] },\r\n",
        "#         'loss_function' : {'values' : ['squared_error']},\r\n",
        "#         'weight_ini' : {'values' : ['random']}\r\n",
        "#     }\r\n",
        "# }\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGjW0ezOxrG9"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XAMlpSNvEa"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU3RfUKixr_a"
      },
      "source": [
        "class NeuralNetwork():\r\n",
        "\r\n",
        "  def __init__(self,n_input,n_output,n_hidden_layers,n_hidden_neurons):\r\n",
        "    self.n_input = n_input\r\n",
        "    self.n_output = n_output\r\n",
        "    self.n_hidden_layers = n_hidden_layers\r\n",
        "    self.n_hidden_neurons = n_hidden_neurons \r\n",
        "    self.layers_size = [self.n_input] + n_hidden_neurons + [self.n_output]  \r\n",
        "    self.lambda1 = None \r\n",
        "    self.eta = None\r\n",
        "    self.batch_size = None\r\n",
        "    self.n_epoch = None\r\n",
        "    self.X_train = None\r\n",
        "    self.Y_train = None\r\n",
        "    self.Y_train_o = None\r\n",
        "    self.X_test = None\r\n",
        "    self.Y_test = None\r\n",
        "    self.Y_test_o = None\r\n",
        "    self.vx = None\r\n",
        "    self.vy = None\r\n",
        "    self.vy_o = None\r\n",
        "    self.arguments = None\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "  def create_network_random(self):\r\n",
        "    # self.total_layers = 2 + self.n_hidden_layers \r\n",
        "    self.W = {}\r\n",
        "    self.b = {} \r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "      self.W[i+1] = np.random.randn(self.layers_size[i],self.layers_size[i+1])\r\n",
        "      self.b[i+1] = np.reshape(np.random.randn(self.layers_size[i+1]),(1,self.layers_size[i+1]))\r\n",
        "    \r\n",
        "    # print(\"W\",len(self.W))\r\n",
        "    # for i in self.W.values():\r\n",
        "    #   print(np.shape(i)) \r\n",
        "    # print(\"self.b\",len(self.b))\r\n",
        "    # for i in self.b.values():\r\n",
        "    #   print(len(i))\r\n",
        "\r\n",
        "  def create_network_xavier(self):\r\n",
        "    # self.total_layers = 2 + self.n_hidden_layers \r\n",
        "    self.W = {}\r\n",
        "    self.b = {}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "      self.W[i+1] = np.random.rand(self.layers_size[i],self.layers_size[i+1]) * np.sqrt(self.layers_size[i]+self.layers_size[i+1])\r\n",
        "      self.b[i+1] =  np.zeros((1,self.layers_size[i+1]))\r\n",
        "   \r\n",
        "  def sigmoid(self,n):\r\n",
        "    n[n<-1e+2] = -1e+2\r\n",
        "    n[n>1e+2] = 1e+2 \r\n",
        "    return 1.0/(1.0 + np.exp(-n))\r\n",
        "\r\n",
        "  # single value n\r\n",
        "  def grad_sigmoid(self,n):\r\n",
        "    return n * (1 - n)\r\n",
        "\r\n",
        "  def Relu(self,n):\r\n",
        "    n[n<0]=0\r\n",
        "    n[n>1e+4] = 1e+4\r\n",
        "    return n\r\n",
        "    \r\n",
        "  def grad_Relu(self,n):\r\n",
        "    n[n<=0] = 0\r\n",
        "    n[n>0] = 1\r\n",
        "    return n\r\n",
        "\r\n",
        "  def tanh(self,n):\r\n",
        "    n[n<-1e+2] = -1e+2\r\n",
        "    n[n>1e+2] = 1e+2 \r\n",
        "    return (np.exp(n) - np.exp(-1*n)) / (np.exp(n) + np.exp(-1*n))\r\n",
        "  \r\n",
        "  def grad_tanh(self,n):\r\n",
        "    return 1-np.power(self.tanh(n),2)\r\n",
        "\r\n",
        "  # a is list\r\n",
        "  def softmax(self,a):\r\n",
        "    a[a>1e+2] = 1e+2\r\n",
        "    a[a<-1e+2]= -1e+2\r\n",
        "    t = np.exp(a)\r\n",
        "    return t/np.sum(t)\r\n",
        "  # y_o original output y_p predicted output\r\n",
        "  def cross_entropy(self,label,pred):\r\n",
        "    t=np.multiply(pred,label)\r\n",
        "    t=t[t!=0]\r\n",
        "    t=-np.log(t)\r\n",
        "    t=t[0]\r\n",
        "    # print(yl)\r\n",
        "    return t + self.regularize_loss()\r\n",
        "  \r\n",
        "\r\n",
        "  #  gradient for the oputput layer when cross entropy is used\r\n",
        "  def grad_cross_entropy(self,y_p,y_o):\r\n",
        "    # print(type((y_p - y_o) ))\r\n",
        "    return (y_p - y_o) \r\n",
        "\r\n",
        "  def regularize_loss(self):\r\n",
        "    total = 0\r\n",
        "    for i in self.W.keys():\r\n",
        "      total += np.sum(np.square(self.W[i]))\r\n",
        "    return self.lambda1 * total\r\n",
        "\r\n",
        "  def squared_error(self,y_o,y_p):\r\n",
        "    return  np.sum((np.array(y_o)-np.array(y_p))**2) + self.regularize_loss()\r\n",
        "\r\n",
        "  def grad_squared_error(self,y_o,y_p):\r\n",
        "    # print(y_o,y_p)\r\n",
        "    y_o = list(y_o)\r\n",
        "    y_p = list(y_p)\r\n",
        "    temp =[]\r\n",
        "    ind = y_o.index(max(y_o))\r\n",
        "    for i in range(len(y_o)):\r\n",
        "      temp.append(-1* 2 * y_p[ind] * (y_o[i]-y_p[ind]) * (y_o[i]-y_p[i]))\r\n",
        "   \r\n",
        "    return np.array(temp)\r\n",
        "\r\n",
        "  def forward_pass(self, x):\r\n",
        "    self.a = {}\r\n",
        "    self.h = {}\r\n",
        "    self.h[0] = x.reshape(1,-1)\r\n",
        "    for i in range(self.n_hidden_layers):\r\n",
        "      # print(np.shape(self.h[i]), np.shape(self.W[i+1]),np.shape(self.b[i+1]))\r\n",
        "      self.a[i+1] = np.matmul(self.h[i], self.tW[i+1]) + self.tb[i+1]\r\n",
        "      # print(self.a[0])\r\n",
        "      self.h[i+1] = getattr(self,self.arguments[0])(self.a[i+1])\r\n",
        "    self.a[self.n_hidden_layers+1] = np.matmul(self.h[self.n_hidden_layers], self.tW[self.n_hidden_layers+1]) + self.tb[self.n_hidden_layers+1]\r\n",
        "    self.h[self.n_hidden_layers+1] = self.softmax(self.a[self.n_hidden_layers+1])\r\n",
        "    return self.h[self.n_hidden_layers+1]\r\n",
        "\r\n",
        "\r\n",
        "  def gradient(self,x,y):\r\n",
        "    self.forward_pass(x)\r\n",
        "    self.d_h = {}\r\n",
        "    self.d_a = {}\r\n",
        "    self.d_a[self.n_hidden_layers+1] = getattr(self,\"grad_\"+self.arguments[2])(self.h[self.n_hidden_layers+1], y)\r\n",
        "    for i in range(self.n_hidden_layers+1, 0, -1):\r\n",
        "      # print(np.shape(self.h[i-1]), np.shape(self.d_a[i]))\r\n",
        "      self.d_w[i] += np.matmul(self.h[i-1].T, self.d_a[i]) + self.tW[i]*self.lambda1\r\n",
        "      self.d_b[i] += self.d_a[i]\r\n",
        "      self.d_h[i-1] = np.matmul(self.d_a[i], self.tW[i].T)\r\n",
        "      self.d_a[i-1] = np.multiply(self.d_h[i-1],getattr(self,\"grad_\"+self.arguments[0])(self.h[i-1]))\r\n",
        "\r\n",
        "\r\n",
        "  # dictionary add key wise\r\n",
        "  def add(self,d1,d2,m1=1,m2=1):\r\n",
        "    temp ={}\r\n",
        "    # print(d1,d2,\"dd\")\r\n",
        "    if (m2==0):\r\n",
        "      return d1\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = m1 * d1[i] + m2 * d2[i]\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def mul(self,d1 , m1 = 1):\r\n",
        "    temp ={}\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = m1 * d1[i]\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def squr(self,d):\r\n",
        "    temp = {}\r\n",
        "    for i in d.keys():\r\n",
        "      temp[i] = d[i]**2\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def adarate(self,d1,d2,n,e):\r\n",
        "    temp = {}\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = (n*d1[i]) /(e+d2[i])**(1/2)\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def onehot_encoding(self,a,n_class):\r\n",
        "    temp = []\r\n",
        "    for i in a:\r\n",
        "      t1 = np.zeros(n_class)\r\n",
        "      t1[i] = 1\r\n",
        "      temp.append(t1)\r\n",
        "    return temp\r\n",
        "\r\n",
        "  #  list of classes ex. [1,2,1,..]\r\n",
        "  def accuracy(self,y_o, y_p):\r\n",
        "    sum = 0\r\n",
        "    for i,j in zip(y_o , y_p):\r\n",
        "      if(i == j):\r\n",
        "        sum = sum + 1\r\n",
        "    return sum/len(y_o)\r\n",
        "\r\n",
        "  def predict_and_loss(self,X,Y,n):\r\n",
        "    loss = 0\r\n",
        "    predicted_class = []\r\n",
        "    for i in range(n):\r\n",
        "      # forward pass\r\n",
        "      pr = self.forward_pass(X[i])[0]\r\n",
        "      pt = list(pr)\r\n",
        "      predicted_class.append(pt.index(max(pr)))\r\n",
        "\r\n",
        "      #  loss\r\n",
        "      # print(i,len(Y))\r\n",
        "      # print(p)\r\n",
        "      ll = getattr(self,self.arguments[2])(Y[i],pt)\r\n",
        "      # print(ll)\r\n",
        "      if(np.isnan(ll) or np.isinf(ll)):\r\n",
        "        loss = loss + 1e+100\r\n",
        "      else:\r\n",
        "        loss = loss + ll\r\n",
        "\r\n",
        "    return predicted_class , loss/n\r\n",
        "\r\n",
        "  def accuracy_and_loss(self,X,Y,y):\r\n",
        "    n_points , _ = np.shape(X)\r\n",
        "    # print(len(Y))\r\n",
        "    p2 , l = self.predict_and_loss(X,Y,n_points)\r\n",
        "    # print(p)\r\n",
        "    acc  = self.accuracy(y, p2)\r\n",
        "\r\n",
        "    return acc , l\r\n",
        "\r\n",
        "  \r\n",
        "  \r\n",
        "\r\n",
        "  def sgd(self):\r\n",
        "    self.d_w , self.d_b = self.oW , self.ob\r\n",
        "    \r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        # print(j)\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "\r\n",
        "        # for k in range(self.n_hidden_layers+1):\r\n",
        "        #   d_w[k] += t1[k]\r\n",
        "        #   d_b[k] += t2[k]\r\n",
        "      \r\n",
        "\r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          # print(j)\r\n",
        "          # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "          for k in self.W.keys():\r\n",
        "            self.W[k] -= self.d_w[k] *self.eta / self.n_points\r\n",
        "            self.b[k] -= self.d_b[k] *self.eta / self.n_points\r\n",
        "          self.tW = self.W\r\n",
        "          self.tb = self.b\r\n",
        "          self.d_w , self.d_b = self.oW , self.ob\r\n",
        "          \r\n",
        "        \r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def mgd(self,gamma = 0.5):\r\n",
        "    self.d_w , self.d_b = self.oW , self.ob\r\n",
        "    \r\n",
        "    p_w , p_b =self.oW , self.ob\r\n",
        "    v_w , v_b =self.oW , self.ob\r\n",
        "\r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "    \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          # print(j)\r\n",
        "          for k in self.W.keys():\r\n",
        "            v_w[k] = p_w[k]*gamma + self.d_w[k]*self.eta\r\n",
        "            v_b[k] = p_b[k] *gamma + self.d_b[k] * self.eta \r\n",
        "            self.W[k] -= v_w[k] /self.n_points\r\n",
        "            self.b[k] -= v_b[k] /self.n_points\r\n",
        "          \r\n",
        "          p_w , p_b = v_w , v_b\r\n",
        "          # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "          self.d_w , self.d_b = self.oW , self.ob\r\n",
        "          self.tW = self.W\r\n",
        "          self.tb = self.b\r\n",
        "        \r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      # wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def ngd(self,gamma = 0.9):\r\n",
        "    self.d_w , self.d_b = self.oW , self.ob\r\n",
        "    p_v_w , p_v_b = self.oW , self.ob\r\n",
        "    v_w , v_b = self.oW , self.ob\r\n",
        "    \r\n",
        "\r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          for k in self.W.keys():\r\n",
        "            v_w[k] = p_v_w[k]*gamma + self.d_w[k] *self.eta\r\n",
        "            v_b[k] = p_v_b[k]*gamma + self.d_b[k] *self.eta\r\n",
        "            self.W[k] -= v_w[k] /self.n_points\r\n",
        "            self.b[k] -= v_b[k] /self.n_points\r\n",
        "            self.tW[k] = self.W[k] - v_w[k] *gamma\r\n",
        "            self.tb[k] = self.b[k] - v_b[k] *gamma \r\n",
        "\r\n",
        "          self.d_w , self.d_b = self.oW , self.ob\r\n",
        "          p_v_w , p_v_b = v_w , v_b\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def adagrad(self,eps = 1e-8):\r\n",
        "    self.d_w , self.d_b = {},{}\r\n",
        "    v_w , v_b = {},{}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "        v_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        self.d_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        self.d_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "    # print(v_w)\r\n",
        "    \r\n",
        "    \r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "\r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          for k in self.W.keys():\r\n",
        "            # print(self.d_w[k])\r\n",
        "            # print(self.d_w[k]**2)\r\n",
        "            v_w[k] += np.square(self.d_w[k])\r\n",
        "            # print(v_w[k])\r\n",
        "            v_b[k] += np.square(self.d_b[k])\r\n",
        "            # print(v_w[k])\r\n",
        "            self.W[k] -= self.eta  * self.d_w[k] / np.sqrt(eps + v_w[k])\r\n",
        "            self.b[k] -= self.eta * self.d_b[k] / np.sqrt(eps + v_b[k])\r\n",
        "\r\n",
        "          \r\n",
        "          self.d_w , self.d_b = {},{}   \r\n",
        "          for k in range(self.n_hidden_layers+1):\r\n",
        "            self.d_w[k+1] = np.zeros((self.layers_size[k], self.layers_size[k+1]))\r\n",
        "            self.d_b[k+1] = np.zeros((1, self.layers_size[k+1]))\r\n",
        "          self.tW = self.W\r\n",
        "          self.tb = self.b\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def rmsprop(self,eps = 1e-8,beta1=0.9):\r\n",
        "    self.d_w , self.d_b = {},{}\r\n",
        "    v_w , v_b = {},{}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "        v_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        self.d_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        self.d_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "   \r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "        \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          for k in self.W.keys():\r\n",
        "            v_w[k] = beta1*v_w[k] + (1-beta1) * self.d_w[k]**2 \r\n",
        "            v_b[k] = beta1*v_b[k] + (1-beta1) * self.d_b[k]**2\r\n",
        "            self.W[k] -= self.d_w[k] *self.eta / (eps + v_w[k])**(1/2)\r\n",
        "            self.b[k] -= self.eta * self.d_b[k] / np.sqrt(eps + v_b[k])\r\n",
        "          self.d_w , self.d_b = {},{}   \r\n",
        "          for k in range(self.n_hidden_layers+1):\r\n",
        "            self.d_w[k+1] = np.zeros((self.layers_size[k], self.layers_size[k+1]))\r\n",
        "            self.d_b[k+1] = np.zeros((1, self.layers_size[k+1]))\r\n",
        "          self.tW = self.W\r\n",
        "          self.tb = self.b\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def adam(self,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "    self.d_w , self.d_b = {},{}\r\n",
        "    v_w , v_b = {},{}\r\n",
        "    m_w , m_b = {},{}\r\n",
        "    m_w_hat ,m_b_hat = {},{}\r\n",
        "    v_w_hat ,v_b_hat = {},{}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "        v_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        self.d_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        self.d_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        m_w[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        m_b[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "        m_w_hat[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        m_b_hat[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "        v_w_hat[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b_hat[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "    \r\n",
        "   \r\n",
        "    time_stamp = 0\r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          for k in self.W.keys():\r\n",
        "            m_w[k] = beta1 * m_w[k] +(1-beta1) * self.d_w[k]\r\n",
        "            m_b[k] = beta1 * m_b[k] +(1-beta1) * self.d_b[k]\r\n",
        "            v_w[k] = beta2 * v_w[k] + (1-beta2) * self.d_w[k]**2\r\n",
        "            v_b[k] = beta2 * v_b[k] + (1-beta2) * self.d_b[k]**2\r\n",
        "            m_w_hat[k] = m_w[k] * 1/(1-beta1**(time_stamp+1))\r\n",
        "            m_b_hat[k] = m_b[k] * 1/(1-beta1**(time_stamp+1))\r\n",
        "            v_w_hat[k] = v_w[k] * 1/(1-beta2**(time_stamp+1))\r\n",
        "            v_b_hat[k] = v_b[k] * 1/(1-beta2**(time_stamp+1))\r\n",
        "            self.W[k] -= m_w_hat[k]*self.eta / (v_w_hat[k] + eps)**(1/2)\r\n",
        "            self.b[k] -= self.eta * m_b_hat[k] / np.sqrt(eps + v_b_hat[k])\r\n",
        "        \r\n",
        "          self.d_w , self.d_b = {},{}   \r\n",
        "          for k in range(self.n_hidden_layers+1):\r\n",
        "            self.d_w[k+1] = np.zeros((self.layers_size[k],self.layers_size[k+1]))\r\n",
        "            self.d_b[k+1] = np.zeros((1, self.layers_size[k+1]))\r\n",
        "          time_stamp = time_stamp+1\r\n",
        "          self.tW = self.W\r\n",
        "          self.tb = self.b\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def nadam(self,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "    self.d_w , self.d_b = {},{}\r\n",
        "    v_w , v_b = {},{}\r\n",
        "    m_w , m_b = {},{}\r\n",
        "    m_w_hat ,m_b_hat = {},{}\r\n",
        "    v_w_hat ,v_b_hat = {},{}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "        v_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        self.d_w[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        self.d_b[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "        m_w[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        m_b[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "        m_w_hat[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        m_b_hat[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "        v_w_hat[i+1] =  np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        v_b_hat[i+1] =  np.zeros((1, self.layers_size[i+1]))\r\n",
        "    time_stamp=0\r\n",
        "\r\n",
        "    for i in tqdm(range(self.n_epoch), total=self.n_epoch, unit=\"epoch\"):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        self.gradient(self.X_train[j],self.Y_train[j])\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          for k in self.W.keys():\r\n",
        "            \r\n",
        "            m_w[k] = beta1 * m_w[k] +(1-beta1) * self.d_w[k]\r\n",
        "            m_b[k] = beta1 * m_b[k] +(1-beta1) * self.d_b[k]\r\n",
        "            v_w[k] = beta2 * v_w[k] + (1-beta2) * self.d_w[k]**2\r\n",
        "            # print(v_w[k])\r\n",
        "            v_b[k] = beta2 * v_b[k] + (1-beta2) * self.d_b[k]**2\r\n",
        "            m_w_hat[k] = m_w[k] * 1/(1-beta1**(time_stamp+1))\r\n",
        "            m_b_hat[k] = m_b[k] * 1/(1-beta1**(time_stamp+1))\r\n",
        "            v_w_hat[k] = v_w[k] * 1/(1-beta2**(time_stamp+1))\r\n",
        "            v_b_hat[k] = v_b[k] * 1/(1-beta2**(time_stamp+1))\r\n",
        "            self.W[k] -= self.eta * (beta1 * m_w_hat[k] + (1-beta1)/(1-beta1**(time_stamp+1)) * self.d_w[k]) / (v_w_hat[k] + eps)**(1/2)\r\n",
        "            self.b[k] -= self.eta *(beta1 * m_b_hat[k] + (1-beta1)/(1-beta1**(time_stamp+1)) * self.d_b[k]) / np.sqrt(eps + v_b_hat[k])\r\n",
        "            self.tW[k] = self.W[k] - beta1 * m_w[k]\r\n",
        "            self.tb[k] = self.b[k] - beta1 * m_b[k]\r\n",
        "\r\n",
        "          self.d_w , self.d_b = {},{}   \r\n",
        "          for k in range(self.n_hidden_layers+1):\r\n",
        "            self.d_w[k+1] = np.zeros((self.layers_size[k],self.layers_size[k+1]))\r\n",
        "            self.d_b[k+1] = np.zeros((1, self.layers_size[k+1]))\r\n",
        "          time_stamp = time_stamp +1\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      # print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "  def fit(self,x_train,y_train,vx,vy,x_test,y_test,arg,optimizer,weight_ini,batch_size,epoch,lambda1,eta,run):\r\n",
        "    self.X_train = x_train\r\n",
        "    self.Y_train = self.onehot_encoding(y_train,10)\r\n",
        "    self.Y_train_o = y_train\r\n",
        "    self.X_test = x_test\r\n",
        "    self.Y_test = self.onehot_encoding(y_test,10)\r\n",
        "    self.Y_test_o = y_test\r\n",
        "    self.vx = vx\r\n",
        "    self.vy = self.onehot_encoding(vy,10)\r\n",
        "    self.vy_o = vy\r\n",
        "    self.arguments = arg\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.n_epoch = epoch\r\n",
        "    self.lambda1 = lambda1\r\n",
        "    self.eta = eta\r\n",
        "    self.n_points , _ = np.shape(self.X_train)\r\n",
        "    self.ob = {}\r\n",
        "    self.oW = {}\r\n",
        "    for i in range(self.n_hidden_layers+1):\r\n",
        "        self.oW[i+1] = np.zeros((self.layers_size[i], self.layers_size[i+1]))\r\n",
        "        self.ob[i+1] = np.zeros((1, self.layers_size[i+1]))\r\n",
        "    getattr(self,\"create_network_\"+weight_ini)()\r\n",
        "    \r\n",
        "    self.tW = self.W\r\n",
        "    self.tb = self.b\r\n",
        "    \r\n",
        "    getattr(self,optimizer)()\r\n",
        "    \r\n",
        "    \r\n",
        "    n_p_t , _ = np.shape(self.X_test) \r\n",
        "    predicted_classes , test_loss = self.predict_and_loss(self.X_test,self.Y_test,n_p_t)\r\n",
        "    # acc = self.accuracy(predicted_classes,self.Y_test_o)\r\n",
        "    # dict = {'train_acc' : t1 , 'train_loss' : t2 ,'val_loss' : t4 , 'val_acc' :t3 , 'test_loss':test_loss,'test_acc':acc }\r\n",
        "    # # np.save(str(run.config.optimizer)+\"_\"+str(run.config.loss_function),D)\r\n",
        "    # df = pd.DataFrame(dict)\r\n",
        "    # df.to_csv(str(run.config.optimizer)+\"_\"+str(run.config.loss_function)+\".csv\")\r\n",
        "\r\n",
        "    # np.save(\"drive/My Drive/DL_assignments/assignment1/weight/\"+str(run.name)+\"W\",self.W)\r\n",
        "    # np.save(\"drive/My Drive/DL_assignments/assignment1/bias/\"+str(run.name)+\"b\",self.b)\r\n",
        "    label = [\"T-shirt/top\" ,\"Trouser\" ,\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "    # wandb.sklearn.plot_confusion_matrix(self.Y_test_o, predicted_classes, label)\r\n",
        "    \r\n",
        "    run.finish()\r\n",
        "    return \r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46i-sgD5NzDy"
      },
      "source": [
        "# ld"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3gBSsAU7JRQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "def load_data():\r\n",
        " \r\n",
        "  (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "  # flatten data\r\n",
        "  x_train = []\r\n",
        "  for j in X_train:\r\n",
        "    x_train.append(j.flatten())\r\n",
        "  x_train=np.array(x_train,dtype = np.float32)/255\r\n",
        "\r\n",
        "  # mean centered\r\n",
        "  temp = []\r\n",
        "  m = np.array(np.mean(x_train,0))\r\n",
        "  for j in x_train:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_train = np.array(temp,dtype = np.float32)\r\n",
        "\r\n",
        "  # print(x_train[0] - x_train[1])\r\n",
        "  x_test = []\r\n",
        "  for j in X_test:\r\n",
        "   x_test.append(j.flatten())\r\n",
        "  x_test=np.array(x_test,dtype = np.float32)/255\r\n",
        "\r\n",
        "  temp = []\r\n",
        "  for j in x_test:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_test = np.array(temp,dtype = np.float32)\r\n",
        "  # print(x_train.shape,x_test.shape)\r\n",
        "\r\n",
        "  x_train , vx_test , y_train , vy_test = train_test_split(x_train,Y_train,test_size=0.10,random_state=12)\r\n",
        "  kk=10\r\n",
        "  return x_train ,y_train, vx_test , vy_test, x_test, Y_test\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDLJiOVEHE_p"
      },
      "source": [
        "def train():\r\n",
        "  run = wandb.init()\r\n",
        "  c = run.config\r\n",
        "  name = \"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activations)+\"_hl_\"+str(c.n_hidden_layers)+\"_hls_\"+str(c.n_hidden_layer_size)+\"_ep_\"+str(c.epochs)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_wi_\"+str(c.weight_ini)\r\n",
        "  run.name = name\r\n",
        "  print(name)\r\n",
        "  \r\n",
        "  hn = [c.n_hidden_layer_size]*c.n_hidden_layers  \r\n",
        "  hl = c.n_hidden_layers \r\n",
        "  l1 = c.weight_decay\r\n",
        "  arg = [c.activations,\"softmax\",c.loss_function]\r\n",
        "  opt = c.optimizer\r\n",
        "  ep = c.epochs\r\n",
        "  bs = c.batch_size\r\n",
        "  lr = c.learning_rate\r\n",
        "  wi = c.weight_ini\r\n",
        "\r\n",
        "  x_train,y_train,vx,vy,x_test,y_test= load_data()\r\n",
        "  n_points , n_input = np.shape(x_train)\r\n",
        "\r\n",
        "  NN = NeuralNetwork(n_input,10,hl,hn)\r\n",
        "  NN.fit(x_train,y_train,vx,vy,x_test,y_test,arg,opt,wi,bs,ep,l1,lr,run)\r\n",
        "\r\n",
        "  return\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI1rxy1HZi25"
      },
      "source": [
        "sweepid= wandb.sweep(sweep_config1,project=\"loss\",entity =\"kkk\")\r\n",
        "wandb.agent(sweepid,train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2BylA1xTR9a"
      },
      "source": [
        "# for i in sweep_config1.parameters.optimizer.values():\r\n",
        "#   s1 = str(i)+\"_cross_entropy.npy\"\r\n",
        "#   s2 = str(i)+\"_squared_error.npy\"\r\n",
        "#   d1 = np.load(s1,allow_pickle=True)\r\n",
        "#   d2 = np.load(s2,allow_pickle=True)\r\n",
        "\r\n",
        "p=np.load(\"nadam_cross_entropy.npy\",allow_pickle=True)\r\n",
        "# print(p['train_acc'])\r\n",
        "# p = dict(p)\r\n",
        "print(type(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW4RGdUnHnr"
      },
      "source": [
        "def trin():\r\n",
        "  # run = wandb.init()\r\n",
        "  # c = run.config\r\n",
        "  # # name = \"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activations)+\"_l_\"+str(c.loss_function)+\"_hl_\"+str(c.n_hidden_layers)+\"_hls_\"+str(c.n_hidden_layer_size)+\"_ep_\"+str(c.epochs)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_wi_\"+str(c.weight_ini)\r\n",
        "  # # run.name = name\r\n",
        "  # # print(name)\r\n",
        "  \r\n",
        "  # hn = [c.n_hidden_layer_size]*c.n_hidden_layers  \r\n",
        "  # hl = c.n_hidden_layers \r\n",
        "  # l1 = c.weight_decay\r\n",
        "  # arg = [c.activations,\"softmax\",c.loss_function]\r\n",
        "  # opt = c.optimizer\r\n",
        "  # ep = c.epochs\r\n",
        "  # bs = c.batch_size\r\n",
        "  # lr = c.learning_rate\r\n",
        "  # wi = c.weight_ini\r\n",
        "\r\n",
        "  hn = [32,32]\r\n",
        "  hl=2\r\n",
        "  l1=0.001\r\n",
        "  arg=[\"sigmoid\",\"softmax\",\"cross_entropy\"]\r\n",
        "  opt = \"mgd\"\r\n",
        "  ep = 5\r\n",
        "  bs = 64\r\n",
        "  lr = 0.001\r\n",
        "  wi = \"random\"\r\n",
        "  run = 3\r\n",
        "\r\n",
        " \r\n",
        "  x_train,y_train,vx,vy,x_test,y_test= load_data()\r\n",
        "  n_points , n_input = np.shape(x_train)\r\n",
        "\r\n",
        "  NN = NeuralNetwork(n_input,10,hl,hn)\r\n",
        "  NN.fit(x_train,y_train,vx,vy,x_test,y_test,arg,opt,wi,bs,ep,l1,lr,run)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  return\r\n",
        "\r\n",
        "trin()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}