{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaushal4 ngd_adagrad",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "17TK1Joi27LD"
      },
      "source": [
        "#Q1\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "dummy = []\r\n",
        "for i in range (10):\r\n",
        "  for j in range(len(Y_train)):\r\n",
        "    if int(Y_train[j]) is i:\r\n",
        "      plt.imshow(X_train[j], cmap=\"gray\")\r\n",
        "      dummy.append(X_train[j])\r\n",
        "      plt.show()\r\n",
        "      break \r\n",
        "wandb.log(({\"Q1\":[wandb.Image(i,caption=j) for i,j in zip(dummy,names)]}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnZlpnCx5GyU"
      },
      "source": [
        "def sgd(run,n_epoch,batch_size=100,eta = 0.01):\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "\r\n",
        "      if(j%batch_size == 0):\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        W = add(W , d_w , 1, -1*eta)\r\n",
        "        b = add(b , d_b , 1, -1*eta)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    # print(W,b)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i) \r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def momentum_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.5):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  \r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "  p_w , p_b =mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b =mul(W , 0) , mul(b,0)\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)   \r\n",
        "    \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_w , d_w , gamma , eta) , add(p_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        p_w , p_b = v_w , v_b\r\n",
        "        # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        \r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcael1Q351mq"
      },
      "source": [
        "def onehot_encoding(a,n_class):   \r\n",
        "  temp = []\r\n",
        "  for i in a:\r\n",
        "    t1 = np.zeros(n_class)\r\n",
        "    t1[i] = 1\r\n",
        "    temp.append(t1)\r\n",
        "  return temp\r\n",
        "\r\n",
        "def squr(d):    \r\n",
        "  temp = {}\r\n",
        "  for i in d.keys():\r\n",
        "    temp[i] = d[i]**2\r\n",
        "  return temp\r\n",
        "\r\n",
        "def mul(d1 , m1 = 1):   \r\n",
        "  temp ={}\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i]\r\n",
        "  return temp\r\n",
        "\r\n",
        "# dictionary add key wise\r\n",
        "def add(d1,d2,m1=1,m2=1):\r\n",
        "  temp ={}\r\n",
        "  # print(d1,d2,\"dd\")\r\n",
        "  if (m2==0):\r\n",
        "    return d1\r\n",
        "  for i in d1.keys():\r\n",
        "    temp[i] = m1 * d1[i] + m2 * d2[i]\r\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeSgluWF6gdi"
      },
      "source": [
        "def nesterov_gradient_descent(run,n_epoch,batch_size=100,eta = 0.001,gamma = 0.9):\r\n",
        "\r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  p_v_w , p_v_b = mul(W , 0) , mul(b,0)\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],add(W,v_w,1,-gamma),add(b,v_b,1,-gamma))\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(p_v_w , d_w , gamma , eta) , add(p_v_b , d_b , gamma , eta)\r\n",
        "        W = add(W , v_w , 1, -1)\r\n",
        "        b = add(b , v_b , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "        p_v_w , p_v_b = v_w , v_b\r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc\r\n",
        "\r\n",
        "def adagrad(run,n_epoch,batch_size=100,eta = 0.001,eps = 1e-8):\r\n",
        "  \r\n",
        "  global W,b,arguments,n_input,n_points ,n_output, n_hidden_layers,n_hidden_neurons , X,Y,X_t,Y_t,y_o,y_t\r\n",
        "  v_w , v_b = mul(W , 0) , mul(b,0)\r\n",
        "  d_w , d_b = mul(W , 0) , mul(b,0)\r\n",
        "  train_loss , train_acc = [] , []\r\n",
        "  val_loss , val_acc = [] , []\r\n",
        "\r\n",
        "  for i in range(n_epoch):\r\n",
        "    for j in range(n_points):\r\n",
        "      t1 , t2 , _ = gradient(X[j],Y[j],W,b)\r\n",
        "      d_w = add(d_w , t1)\r\n",
        "      d_b = add(d_b , t2)\r\n",
        "      \r\n",
        "      if(j%batch_size == 0):\r\n",
        "        v_w , v_b = add(v_w, squr(d_w)) , add(v_b , squr(d_b))\r\n",
        "        W = add(W , adarate(d_w,v_w,eta,eps) , 1, -1)\r\n",
        "        b = add(b , adarate(d_b,v_b,eta,eps) , 1 , -1)\r\n",
        "        d_w , d_b = mul(W , 0) , mul(b,0)    \r\n",
        "\r\n",
        "    # train acc and loss\r\n",
        "    t4 , t5 = accuracy_and_loss(X,Y,y_o)\r\n",
        "    train_loss.append(t5)\r\n",
        "    train_acc.append(t4)\r\n",
        "\r\n",
        "    # validate\r\n",
        "    va , vl = accuracy_and_loss(X_t,Y_t,y_t)\r\n",
        "    val_loss.append(vl)\r\n",
        "    val_acc.append(va)\r\n",
        "    print(\"epoch\",i,\"trian acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "    # wandb.log({'train_acc' : train_acc[-1] , 'train_loss' : train_loss[-1] , 'val_acc' : val_acc[-1], 'val_loss': val_loss[-1]})\r\n",
        "    # wandb.log({'train_acc' : t4 , 'train_loss' : t5},step = i)\r\n",
        "    vl = float (vl)\r\n",
        "    t5 = float (t5)\r\n",
        "    run.log({'t_acc' : t4 ,'t_loss' : t5,'v_loss' : vl,'v_acc':va})\r\n",
        "  return W , b , train_loss , train_acc , val_loss, val_acc "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}