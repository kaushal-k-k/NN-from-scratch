{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXAktufaNqwZ"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BddwesMxjG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740ed3a1-5366-4a03-cb33-40e27a5b3064"
      },
      "source": [
        "%pip install wandb -q\r\n",
        "import wandb\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "random.seed(1)\r\n",
        "sweep_config={\r\n",
        "    'method' : 'random' ,\r\n",
        "    'metric' : { 'name' : 'val_acc' , 'goal' : 'maximize' } ,\r\n",
        "    'parameters' : {\r\n",
        "        'epochs' : { 'values' : [5,6,10] },\r\n",
        "        'n_hidden_layers' : {'values' : [3,4,2]},\r\n",
        "        'n_hidden_layer_size' : { 'values' : [32,64,128]},\r\n",
        "        'batch_size' : { 'values' : [16,32,64]},\r\n",
        "        'weight_decay' : { 'values' : [0,0.0005] },\r\n",
        "        'learning_rate' : { 'values' : [1e-3 ,5e-3,1e-4] },\r\n",
        "        'optimizer' : { 'values' : ['sgd','ngd','mgd','rmsprop','adam','nadam','adagrad'] },\r\n",
        "        'activations' : { 'values' : ['sigmoid','tanh','Relu'] },\r\n",
        "        'loss_function' : {'values' : ['cross_entropy' , 'squared_error']},\r\n",
        "        'weight_ini' : {'values' : ['random' , 'xavier']}\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 5.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 20.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 24.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGjW0ezOxrG9",
        "outputId": "d996f6f4-b336-4397-e731-c303bbf3ba53"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XAMlpSNvEa"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU3RfUKixr_a"
      },
      "source": [
        "class NeuralNetwork():\r\n",
        "\r\n",
        "  def __init__(self,n_input,n_output,n_hidden_layers,n_hidden_neurons):\r\n",
        "    self.n_input = n_input\r\n",
        "    self.n_output = n_output\r\n",
        "    self.n_hidden_layers = n_hidden_layers\r\n",
        "    self.n_hidden_neurons = n_hidden_neurons  \r\n",
        "    self.lambda1 = None \r\n",
        "    self.eta = None\r\n",
        "    self.batch_size = None\r\n",
        "    self.n_epoch = None\r\n",
        "    self.X_train = None\r\n",
        "    self.Y_train = None\r\n",
        "    self.Y_train_o = None\r\n",
        "    self.X_test = None\r\n",
        "    self.Y_test = None\r\n",
        "    self.Y_test_o = None\r\n",
        "    self.vx = None\r\n",
        "    self.vy = None\r\n",
        "    self.vy_o = None\r\n",
        "    self.arguments = None\r\n",
        "    # print(self.n_hidden_neurons)\r\n",
        "\r\n",
        "  def create_network_random(self):\r\n",
        "    self.total_layers = 2 + self.n_hidden_layers \r\n",
        "    self.W = {}\r\n",
        "    self.b = {}\r\n",
        "    \r\n",
        "    # initialization for W0 and b0 i.e. input layer\r\n",
        "    self.W[0] = np.random.randn(self.n_input,self.n_hidden_neurons[0])\r\n",
        "    self.b[0] = np.random.randn(self.n_hidden_neurons[0])\r\n",
        "    # print(type(W[0]))\r\n",
        "\r\n",
        "    # hidden layer\r\n",
        "    for i in range(1,self.n_hidden_layers):\r\n",
        "      self.W[i] = np.random.randn(self.n_hidden_neurons[i-1],self.n_hidden_neurons[i])\r\n",
        "      self.b[i] = np.random.randn(self.n_hidden_neurons[i])\r\n",
        "    \r\n",
        "    # output layer\r\n",
        "    self.W[self.total_layers-2] = np.random.randn(self.n_hidden_neurons[-1],self.n_output)\r\n",
        "    self.b[self.total_layers-2] = np.random.randn(self.n_output)\r\n",
        "\r\n",
        "    # print(\"W\",len(self.W))\r\n",
        "    # for i in self.W.values():\r\n",
        "    #   print(np.shape(i)) \r\n",
        "    # print(\"self.b\",len(self.b))\r\n",
        "    # for i in self.b.values():\r\n",
        "    #   print(len(i))\r\n",
        "\r\n",
        "  def create_network_xavier(self):\r\n",
        "    self.total_layers = 2 + self.n_hidden_layers \r\n",
        "    self.W = {}\r\n",
        "    self.b = {}\r\n",
        "    \r\n",
        "    # initialization for W0 and b0 i.e. input layer\r\n",
        "    self.W[0] = np.random.rand(self.n_input,self.n_hidden_neurons[0]) * np.sqrt((self.n_input+self.n_hidden_neurons[0]))\r\n",
        "    self.b[0] = np.random.rand(self.n_hidden_neurons[0]) * 0\r\n",
        "    # print(type(W[0]))\r\n",
        "\r\n",
        "    # hidden layer\r\n",
        "    for i in range(1,self.n_hidden_layers):\r\n",
        "      self.W[i] = np.random.rand(self.n_hidden_neurons[i-1],self.n_hidden_neurons[i]) * np.sqrt((self.n_hidden_neurons[i-1] + self.n_hidden_neurons[i]))\r\n",
        "      self.b[i] = np.random.rand(self.n_hidden_neurons[i]) * 0\r\n",
        "    \r\n",
        "    # output layer\r\n",
        "    self.W[self.total_layers-2] = np.random.rand(self.n_hidden_neurons[-1],self.n_output) * np.sqrt((self.n_hidden_neurons[-1] + self.n_output))\r\n",
        "    self.b[self.total_layers-2] = np.random.rand(self.n_output) *0\r\n",
        "\r\n",
        "  def sigmoid(self,n):\r\n",
        "    if(n >= 1e+2 ):\r\n",
        "      return 1\r\n",
        "    elif (n <= 1e-2):\r\n",
        "      return 1e-3\r\n",
        "    return 1/(1 + np.exp(-n,dtype= np.float128))\r\n",
        "\r\n",
        "  # single value n\r\n",
        "  def grad_sigmoid(self,n):\r\n",
        "    temp = self.sigmoid(n)\r\n",
        "    return temp * (1 - temp)\r\n",
        "\r\n",
        "  def Relu(self,n):\r\n",
        "    if n <= 0:\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return n\r\n",
        "    \r\n",
        "  def grad_Relu(self,n):\r\n",
        "    if n <= 0:\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "\r\n",
        "  def tanh(self,n):\r\n",
        "    if ( n >= 1e+2):\r\n",
        "      return 1\r\n",
        "    elif (n <= -1e+2):\r\n",
        "      return -1\r\n",
        "    else:\r\n",
        "      return (np.exp(n,dtype=np.float128) - np.exp(-1*n)) / (np.exp(n) + np.exp(-1*n))\r\n",
        "    return 0\r\n",
        "  \r\n",
        "  def grad_tanh(self,n):\r\n",
        "    return 1-np.power(self.tanh(n),2)\r\n",
        "\r\n",
        "  # a is list\r\n",
        "  def softmax(self,a):\r\n",
        "    p = []\r\n",
        "    for i in a:\r\n",
        "      if(i <= 1e+4 ):\r\n",
        "        if(i >= -1e+4):\r\n",
        "          p.append(np.exp(i,dtype=np.float128))\r\n",
        "        else:\r\n",
        "          p.append(0)\r\n",
        "      else:\r\n",
        "        return a/np.sum(a)\r\n",
        "    if(np.sum(p) == 0):\r\n",
        "      return p\r\n",
        "    return p/np.sum(p)\r\n",
        "\r\n",
        "  # y_o original output y_p predicted output\r\n",
        "  def cross_entropy(self,y_o,y_p):\r\n",
        "    for i,j in zip(y_o,y_p):\r\n",
        "      if (i==1 and j >= 0):\r\n",
        "        return -1*np.log(1e-15+j,dtype=np.float128) + self.regularize_loss()\r\n",
        "    return 1e+3 \r\n",
        "\r\n",
        "  #  gradient for the oputput layer when cross entropy is used\r\n",
        "  def grad_cross_entropy(self,y_o,y_p):\r\n",
        "    return -(y_o - y_p) \r\n",
        "\r\n",
        "  def regularize_loss(self):\r\n",
        "    temp = self.squr(self.W)\r\n",
        "    # print(temp)\r\n",
        "    total = 0\r\n",
        "    for i in temp.keys():\r\n",
        "      total = total + np.sum(temp[i])\r\n",
        "\r\n",
        "    return self.lambda1 * total\r\n",
        "\r\n",
        "  def squared_error(self,y_o,y_p):\r\n",
        "    return  np.sum((np.array(y_o)-np.array(y_p))**2) + self.regularize_loss()\r\n",
        "\r\n",
        "  def grad_squared_error(self,y_o,y_p):\r\n",
        "    # print(y_o,y_p)\r\n",
        "    y_o = list(y_o)\r\n",
        "    y_p = list(y_p)\r\n",
        "    temp =[]\r\n",
        "    ind = y_o.index(max(y_o))\r\n",
        "    for i in range(len(y_o)):\r\n",
        "      temp.append(-1* 2 * y_p[ind] * (y_o[i]-y_p[ind]) * (y_o[i]-y_p[i]))\r\n",
        "    return np.array(temp)\r\n",
        "\r\n",
        "  def forward_pass(self,X,W,b):\r\n",
        "    # h does not contain X\r\n",
        "    h={}\r\n",
        "    a={}\r\n",
        "    # input \r\n",
        "    a[0] = np.array(X @ W[0] + b[0],dtype=np.float32)\r\n",
        "    # print(a[0])\r\n",
        "  \r\n",
        "  \r\n",
        "    h[0] = list(map(lambda x :getattr(self,self.arguments[0])(x),a[0]))\r\n",
        "    # print(h[0])\r\n",
        "\r\n",
        "    # hidden\r\n",
        "    for i in range(1,self.n_hidden_layers):\r\n",
        "      a[i] = np.array(h[i-1] @ W[i] + b[i])\r\n",
        "      h[i] = list(map(lambda x : getattr(self,self.arguments[0])(x),a[i]))\r\n",
        "\r\n",
        "    # output\r\n",
        "    a[self.n_hidden_layers] = np.array(h[self.n_hidden_layers-1] @ W[self.n_hidden_layers] + b[self.n_hidden_layers])\r\n",
        "\r\n",
        "    # print(\"a\",a,\"h\")\r\n",
        "\r\n",
        "    return a , h\r\n",
        "\r\n",
        "  def backward_pass(self,x,y,p,a,h,W,b):\r\n",
        "    \r\n",
        "    d_al = None\r\n",
        "    d_h = None\r\n",
        "    inner_activation = self.arguments[0]\r\n",
        "    output_activation = self.arguments[2]\r\n",
        "\r\n",
        "    # gradient for output w.r.t a_l\r\n",
        "    # print(\"o\",y)\r\n",
        "    d_al = getattr(self,\"grad_\"+output_activation)(y,p)\r\n",
        "    # print(self.n_hidden_layers)\r\n",
        "    d_w ={}\r\n",
        "    d_b ={}\r\n",
        "    #  for all hidden layers\r\n",
        "    # print(self.n_hidden_layers)\r\n",
        "    for i in range(self.n_hidden_layers,0,-1):\r\n",
        "      d_w[i] = np.array([np.dot(d_al,h[i-1][j]) for j in range(self.n_hidden_neurons[i-1])])\r\n",
        "    \r\n",
        "      d_b[i] = np.array(d_al)\r\n",
        "\r\n",
        "      d_h = np.array((np.matrix(W[i]) @ np.matrix(d_al).T).T)[0]\r\n",
        "      # print(\"d_h\",d_h)\r\n",
        "\r\n",
        "      d_al = [d_h[j] * getattr(self,\"grad_\"+inner_activation)(a[i-1][j]) for j in range(self.n_hidden_neurons[i-1])]\r\n",
        "\r\n",
        "    # for input layer\r\n",
        "    d_w[0] = np.array([np.dot(d_al,x[j]) for j in range(self.n_input)])\r\n",
        "    d_b[0] =  np.array(d_al)\r\n",
        "\r\n",
        "    return self.add(d_w,W,1,self.lambda1) , d_b\r\n",
        "\r\n",
        "  def gradient(self,x,y,W,b):\r\n",
        "    # forward pass\r\n",
        "    a , h = self.forward_pass(x,W,b)\r\n",
        "    # print(a,h)\r\n",
        "    # output activation\r\n",
        "    p = getattr(self,self.arguments[1])(a[self.n_hidden_layers])\r\n",
        "\r\n",
        "    # loss\r\n",
        "    l = getattr(self,self.arguments[2])(y,p)\r\n",
        "\r\n",
        "    # backward pass\r\n",
        "    d_w ,d_b = self.backward_pass(x,y,p,a,h,W,b)\r\n",
        "    # print(d_w , d_b)\r\n",
        "\r\n",
        "    return d_w , d_b , l\r\n",
        "\r\n",
        "  # dictionary add key wise\r\n",
        "  def add(self,d1,d2,m1=1,m2=1):\r\n",
        "    temp ={}\r\n",
        "    # print(d1,d2,\"dd\")\r\n",
        "    if (m2==0):\r\n",
        "      return d1\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = m1 * d1[i] + m2 * d2[i]\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def mul(self,d1 , m1 = 1):\r\n",
        "    temp ={}\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = m1 * d1[i]\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def squr(self,d):\r\n",
        "    temp = {}\r\n",
        "    for i in d.keys():\r\n",
        "      temp[i] = d[i]**2\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def adarate(self,d1,d2,n,e):\r\n",
        "    temp = {}\r\n",
        "    for i in d1.keys():\r\n",
        "      temp[i] = (n*d1[i]) /(e+d2[i])**(1/2)\r\n",
        "    return temp\r\n",
        "\r\n",
        "  def onehot_encoding(self,a,n_class):\r\n",
        "    temp = []\r\n",
        "    for i in a:\r\n",
        "      t1 = np.zeros(n_class)\r\n",
        "      t1[i] = 1\r\n",
        "      temp.append(t1)\r\n",
        "    return temp\r\n",
        "\r\n",
        "  #  list of classes ex. [1,2,1,..]\r\n",
        "  def accuracy(self,y_o, y_p):\r\n",
        "    sum = 0\r\n",
        "    for i,j in zip(y_o , y_p):\r\n",
        "      if(i == j):\r\n",
        "        sum = sum + 1\r\n",
        "    return sum/len(y_o)\r\n",
        "\r\n",
        "  def predict_and_loss(self,X,Y,n):\r\n",
        "    loss = 0\r\n",
        "    predicted_class = []\r\n",
        "    for i in range(n):\r\n",
        "      # forward pass\r\n",
        "      a , h = self.forward_pass(X[i],self.W,self.b)\r\n",
        "      # if(i==0):\r\n",
        "      #   print(\"a\",a,\"h\",h)\r\n",
        "      # output activation\r\n",
        "      p = getattr(self,self.arguments[1])(a[self.n_hidden_layers])\r\n",
        "      p = list(p)\r\n",
        "      # print(p)\r\n",
        "      predicted_class.append(p.index(max(p)))\r\n",
        "\r\n",
        "      #  loss\r\n",
        "      # print(i,len(Y))\r\n",
        "      # print(p)\r\n",
        "      ll = getattr(self,self.arguments[2])(Y[i],p)\r\n",
        "      # print(ll)\r\n",
        "      if(np.isnan(ll) or np.isinf(ll)):\r\n",
        "        loss = loss + 1e+100\r\n",
        "      else:\r\n",
        "        loss = loss + ll\r\n",
        "\r\n",
        "    return predicted_class , loss/n\r\n",
        "\r\n",
        "  def accuracy_and_loss(self,X,Y,y):\r\n",
        "    n_points , _ = np.shape(X)\r\n",
        "    # print(len(Y))\r\n",
        "    p , l = self.predict_and_loss(X,Y,n_points)\r\n",
        "    # print(p)\r\n",
        "    acc  = self.accuracy(y, p)\r\n",
        "\r\n",
        "    return acc , l\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  def sgd(self):\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "   \r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.W,self.b)\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "\r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "          self.W = self.add(self.W , d_w , 1, -1*self.eta)\r\n",
        "          self.b = self.add(self.b , d_b , 1, -1*self.eta)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "        \r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def mgd(self,gamma = 0.5):\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    p_w , p_b =self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    v_w , v_b =self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "\r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.W,self.b)\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)   \r\n",
        "    \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          v_w , v_b = self.add(p_w , d_w , gamma , self.eta) , self.add(p_b , d_b , gamma , self.eta)\r\n",
        "          self.W = self.add(self.W , v_w , 1, -1)\r\n",
        "          self.b = self.add(self.b , v_b , 1 , -1)\r\n",
        "          p_w , p_b = v_w , v_b\r\n",
        "          # print(\"dw\",d_w,\"db\",d_b)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "        \r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def ngd(self,gamma = 0.9):\r\n",
        "    p_v_w , p_v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    v_w , v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "\r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.add(self.W,v_w,1,-gamma),self.add(self.b,v_b,1,-gamma))\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          v_w , v_b = self.add(p_v_w , d_w , gamma , self.eta) , self.add(p_v_b , d_b , gamma , self.eta)\r\n",
        "          self.W = self.add(self.W , v_w , 1, -1)\r\n",
        "          self.b = self.add(self.b , v_b , 1 , -1)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "          p_v_w , p_v_b = v_w , v_b\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def adagrad(self,eps = 1e-8):\r\n",
        "    v_w , v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    \r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.W,self.b)\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          v_w , v_b = self.add(v_w, self.squr(d_w)) , self.add(v_b , self.squr(d_b))\r\n",
        "          self.W = self.add(self.W , self.adarate(d_w,v_w,self.eta,eps) , 1, -1)\r\n",
        "          self.b = self.add(self.b , self.adarate(d_b,v_b,self.eta,eps) , 1 , -1)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)    \r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def rmsprop(self,eps = 1e-8,beta1=0.9):\r\n",
        "    v_w , v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "\r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.W,self.b)\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          v_w , v_b = self.add(v_w, self.squr(d_w),beta1,(1-beta1)) , self.add(v_b , self.squr(d_b),beta1,(1-beta1))\r\n",
        "          self.W = self.add(self.W , self.adarate(d_w,v_w,self.eta,eps) , 1, -1)\r\n",
        "          self.b = self.add(self.b , self.adarate(d_b,v_b,self.eta,eps) , 1 , -1)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def adam(self,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "    v_w , v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    m_w , m_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    time_stamp = 0\r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.W,self.b)\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "        \r\n",
        "          m_w , m_b = self.add(m_w,d_w,beta1 ,(1-beta1)) , self.add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "          v_w , v_b = self.add(v_w, self.squr(d_w),beta2,(1-beta2)) , self.add(v_b , self.squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "          m_w_hat , m_b_hat = self.mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  self.mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "          v_w_hat , v_b_hat = self.mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  self.mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "          self.W = self.add(self.W , self.adarate(m_w_hat,v_w_hat,self.eta,eps) , 1, -1)\r\n",
        "          self.b = self.add(self.b , self.adarate(m_b_hat,v_b_hat,self.eta,eps) , 1 , -1)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "          time_stamp = time_stamp+1\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "\r\n",
        "  def nadam(self,eps = 1e-8,beta1 = 0.9,beta2 =0.999):\r\n",
        "    v_w , v_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    m_w , m_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "    time_stamp=0\r\n",
        "    for i in range(self.n_epoch):\r\n",
        "      for j in range(self.n_points):\r\n",
        "        t1 , t2 , _ = self.gradient(self.X_train[j],self.Y_train[j],self.add(self.W,m_w,1,-beta1),self.add(self.b,m_b,1,-beta1))\r\n",
        "        d_w = self.add(d_w , t1)\r\n",
        "        d_b = self.add(d_b , t2)\r\n",
        "      \r\n",
        "        if(j%self.batch_size == 0):\r\n",
        "          m_w , m_b = self.add(m_w,d_w,beta1 ,(1-beta1)) , self.add(m_b,d_b,beta1 ,(1-beta1)) \r\n",
        "          v_w , v_b = self.add(v_w, self.squr(d_w),beta2,(1-beta2)) , self.add(v_b , self.squr(d_b),beta2,(1-beta2))\r\n",
        "    \r\n",
        "          m_w_hat , m_b_hat = self.mul(m_w , 1/(1-beta1**(time_stamp+1))) ,  self.mul(m_b , 1/(1-beta1**(time_stamp+1)))\r\n",
        "          v_w_hat , v_b_hat = self.mul(v_w , 1/(1-beta2**(time_stamp+1))) ,  self.mul(v_b , 1/(1-beta2**(time_stamp+1)))  \r\n",
        "\r\n",
        "          self.W = self.add(self.W , self.adarate(self.add(m_w_hat,d_w,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_w_hat,self.eta,eps) , 1, -1)\r\n",
        "          self.b = self.add(self.b , self.adarate(self.add(m_b_hat,d_b,beta1,(1-beta1)/(1-beta1**(time_stamp+1))),v_b_hat,self.eta,eps) , 1 , -1)\r\n",
        "          d_w , d_b = self.mul(self.W , 0) , self.mul(self.b,0)\r\n",
        "          time_stamp = time_stamp +1\r\n",
        "\r\n",
        "      # train acc and loss\r\n",
        "      t4 , t5 = self.accuracy_and_loss(self.X_train,self.Y_train,self.Y_train_o)\r\n",
        "      t4,t5 = float(t4),float(t5)\r\n",
        "      # validate\r\n",
        "      va , vl = self.accuracy_and_loss(self.vx,self.vy,self.vy_o)\r\n",
        "      va,vl = float(va),float(vl)\r\n",
        "      wandb.log({'train_acc' : t4 , 'train_loss' : t5 , 'val_acc' : va, 'val_loss': vl })\r\n",
        "      print(\"epoch\",i,\"train acc\", t4 , \"train loss\" ,t5 , \"validation acc\" , va , \"validation loss\" ,vl)\r\n",
        "        \r\n",
        "\r\n",
        "  def fit(self,x_train,y_train,vx,vy,x_test,y_test,arg,optimizer,weight_ini,batch_size,epoch,lambda1,eta,run):\r\n",
        "    self.X_train = x_train\r\n",
        "    self.Y_train = self.onehot_encoding(y_train,10)\r\n",
        "    self.Y_train_o = y_train\r\n",
        "    self.X_test = x_test\r\n",
        "    self.Y_test = self.onehot_encoding(y_test,10)\r\n",
        "    self.Y_test_o = y_test\r\n",
        "    self.vx = vx\r\n",
        "    self.vy = self.onehot_encoding(vy,10)\r\n",
        "    self.vy_o = vy\r\n",
        "    self.arguments = arg\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.n_epoch = epoch\r\n",
        "    self.lambda1 = lambda1\r\n",
        "    self.eta = eta\r\n",
        "    self.n_points , _ = np.shape(self.X_train)\r\n",
        "\r\n",
        "    getattr(self,\"create_network_\"+weight_ini)()\r\n",
        "    getattr(self,optimizer)()\r\n",
        "\r\n",
        "    n_p_t , _ = np.shape(self.X_test) \r\n",
        "    predicted_classes , test_loss = self.predict_and_loss(self.X_test,self.Y_test,n_p_t)\r\n",
        "    np.save(\"drive/My Drive/DL_assignments/assignment1/weight/\"+str(run.name)+\"W\",self.W)\r\n",
        "    np.save(\"drive/My Drive/DL_assignments/assignment1/bias/\"+str(run.name)+\"b\",self.b)\r\n",
        "    label = [\"T-shirt/top\" ,\"Trouser\" ,\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "    wandb.sklearn.plot_confusion_matrix(self.Y_test_o, predicted_classes, label)\r\n",
        "    \r\n",
        "    run.finish()\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46i-sgD5NzDy"
      },
      "source": [
        "# ld"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3gBSsAU7JRQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "def load_data():\r\n",
        " \r\n",
        "  (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "  # flatten data\r\n",
        "  x_train = []\r\n",
        "  for j in X_train:\r\n",
        "    x_train.append(j.flatten())\r\n",
        "  x_train=np.array(x_train,dtype = np.float32)/255\r\n",
        "\r\n",
        "  # mean centered\r\n",
        "  temp = []\r\n",
        "  m = np.array(np.mean(x_train,0))\r\n",
        "  for j in x_train:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_train = np.array(temp,dtype = np.float32)\r\n",
        "\r\n",
        "  # print(x_train[0] - x_train[1])\r\n",
        "  x_test = []\r\n",
        "  for j in X_test:\r\n",
        "   x_test.append(j.flatten())\r\n",
        "  x_test=np.array(x_test,dtype = np.float32)/255\r\n",
        "\r\n",
        "  temp = []\r\n",
        "  for j in x_test:\r\n",
        "    temp.append(np.array(j)-m)\r\n",
        "  x_test = np.array(temp,dtype = np.float32)\r\n",
        "  # print(x_train.shape,x_test.shape)\r\n",
        "\r\n",
        "  x_train , vx_test , y_train , vy_test = train_test_split(x_train,Y_train,test_size=0.10,random_state=12)\r\n",
        "  return x_train ,y_train, vx_test , vy_test, x_test, Y_test\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDLJiOVEHE_p"
      },
      "source": [
        "def train():\r\n",
        "  run = wandb.init()\r\n",
        "  c = run.config\r\n",
        "  name = \"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activations)+\"_l_\"+str(c.loss_function)+\"_hl_\"+str(c.n_hidden_layers)+\"_hls_\"+str(c.n_hidden_layer_size)+\"_ep_\"+str(c.epochs)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_wi_\"+str(c.weight_ini)\r\n",
        "  run.name = name\r\n",
        "  print(name)\r\n",
        "  \r\n",
        "  hn = [c.n_hidden_layer_size]*c.n_hidden_layers  \r\n",
        "  hl = c.n_hidden_layers \r\n",
        "  l1 = c.weight_decay\r\n",
        "  arg = [c.activations,\"softmax\",c.loss_function]\r\n",
        "  opt = c.optimizer\r\n",
        "  ep = c.epochs\r\n",
        "  bs = c.batch_size\r\n",
        "  lr = c.learning_rate\r\n",
        "  wi = c.weight_ini\r\n",
        "\r\n",
        "  x_train,y_train,vx,vy,x_test,y_test= load_data()\r\n",
        "  n_points , n_input = np.shape(x_train)\r\n",
        "\r\n",
        "  NN = NeuralNetwork(n_input,10,hl,hn)\r\n",
        "  NN.fit(x_train,y_train,vx,vy,x_test,y_test,arg,opt,wi,bs,ep,l1,lr,run)\r\n",
        "\r\n",
        "  return\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "NI1rxy1HZi25",
        "outputId": "d047e1f9-d670-4de4-f543-8ab64969fba1"
      },
      "source": [
        "sweepid= wandb.sweep(sweep_config,project=\"final\",entity =\"sonagara\")\r\n",
        "wandb.agent(sweepid,train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: egi13voh\n",
            "Sweep URL: https://wandb.ai/sonagara/final/sweeps/egi13voh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fctq08ow with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivations: Relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: squared_error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden_layer_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ini: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msonagara\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">worldly-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sonagara/final\" target=\"_blank\">https://wandb.ai/sonagara/final</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/sonagara/final/sweeps/egi13voh\" target=\"_blank\">https://wandb.ai/sonagara/final/sweeps/egi13voh</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/sonagara/final/runs/fctq08ow\" target=\"_blank\">https://wandb.ai/sonagara/final/runs/fctq08ow</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210312_140338-fctq08ow</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "op_adagrad_ac_Relu_l_squared_error_hl_4_hls_128_ep_1_n_0.0001_bs_32_wi_random\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYW4RGdUnHnr"
      },
      "source": [
        "##for training without wandb\r\n",
        "#  def trin():\r\n",
        "#   # run = wandb.init()\r\n",
        "#   # c = run.config\r\n",
        "#   # # name = \"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activations)+\"_l_\"+str(c.loss_function)+\"_hl_\"+str(c.n_hidden_layers)+\"_hls_\"+str(c.n_hidden_layer_size)+\"_ep_\"+str(c.epochs)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_wi_\"+str(c.weight_ini)\r\n",
        "#   # # run.name = name\r\n",
        "#   # # print(name)\r\n",
        "  \r\n",
        "#   # hn = [c.n_hidden_layer_size]*c.n_hidden_layers  \r\n",
        "#   # hl = c.n_hidden_layers \r\n",
        "#   # l1 = c.weight_decay\r\n",
        "#   # arg = [c.activations,\"softmax\",c.loss_function]\r\n",
        "#   # opt = c.optimizer\r\n",
        "#   # ep = c.epochs\r\n",
        "#   # bs = c.batch_size\r\n",
        "#   # lr = c.learning_rate\r\n",
        "#   # wi = c.weight_ini\r\n",
        "\r\n",
        "#   hn = [12,12]\r\n",
        "#   hl=2\r\n",
        "#   l1=0.001\r\n",
        "#   arg=[\"sigmoid\",\"softmax\",\"cross_entropy\"]\r\n",
        "#   opt = \"nadam\"\r\n",
        "#   ep = 3\r\n",
        "#   bs = 32\r\n",
        "#   lr = 0.001\r\n",
        "#   wi = \"random\"\r\n",
        "#   run = 3\r\n",
        "\r\n",
        " \r\n",
        "#   x_train,y_train,vx,vy,x_test,y_test= load_data()\r\n",
        "#   n_points , n_input = np.shape(x_train)\r\n",
        "\r\n",
        "#   NN = NeuralNetwork(n_input,10,hl,hn)\r\n",
        "#   NN.fit(x_train,y_train,vx,vy,x_test,y_test,arg,opt,wi,bs,ep,l1,lr,run)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#   return\r\n",
        "\r\n",
        "# trin()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}